{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e374663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import forestplot as fp\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.power as smp\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "import import_ipynb\n",
    "from apriori_power_analysis import calculate_target_cohens_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_assumption_checks(df, independent_var_col, dependent_var_col, robust_std_errors=False):\n",
    "    \"\"\"\n",
    "    Perform assumption checks for a multiple linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (pd.DataFrame): The input dataset containing the dependent and independent variables.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # --- 1. Prepare the Data ---\n",
    "    print(\"\\n--- Preparing Data for Linear Regression ---\")\n",
    "\n",
    "    y = df[dependent_var_col]\n",
    "    X = df[independent_var_col]\n",
    "    X = sm.add_constant(X) # Add a constant (intercept) to the model\n",
    "\n",
    "\n",
    "    # --- 2. Fit the Multiple Linear Regression Model ---\n",
    "    if robust_std_errors:\n",
    "        print(\"\\nFitting model with robust standard errors (HC3)...\")\n",
    "        model = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "    else:\n",
    "        print(\"\\nFitting model with standard OLS...\")\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Extract fitted values and residuals\n",
    "    fitted_vals = model.fittedvalues\n",
    "    residuals = model.resid\n",
    "\n",
    "    # --- 3. Check for Linearity & 4. Check for Homoscedasticity ---\n",
    "    # These can be checked together using the Residuals vs. Fitted plot.\n",
    "\n",
    "    print(\"\\n--- 1. Checking Linearity and 2. Homoscedasticity ---\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.residplot(x=fitted_vals, y=residuals, lowess=True,\n",
    "                line_kws={'color': 'red', 'lw': 2},\n",
    "                scatter_kws={'alpha': 0.5})\n",
    "    plt.title('Residuals vs. Fitted Values')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Interpretation of Residuals vs. Fitted Plot:\")\n",
    "    print(\"1. Linearity: The red line should be close to horizontal at 0. A curve indicates non-linearity.\")\n",
    "    print(\"2. Homoscedasticity: The points should be randomly scattered in a constant-width band. A funnel shape indicates heteroscedasticity.\")\n",
    "\n",
    "\n",
    "    # Check Linearity with Partial Regression Plots\n",
    "    print(\"\\n--- ASSUMPTION 1: Linearity with Partial Regression Plots ---\")\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    sm.graphics.plot_partregress_grid(model, fig=fig)\n",
    "    fig.suptitle(\"Partial Regression Plots\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Interpretation: Each plot shows the relationship between y and one predictor,\")\n",
    "    print(\"holding other predictors constant. Look for a linear trend in each subplot.\")\n",
    "\n",
    "\n",
    "    # Homoscedasticity Test\n",
    "    print(\"\\n--- ASSUMPTION 2: Homoscedasticity Test ---\")\n",
    "    bp_stat2, bp_p2, _, _ = het_breuschpagan(model.resid, X)\n",
    "    HOMOSCEDASTICITY_GOOD = \"✅\" if bp_p2 > 0.05 else \"❌\"\n",
    "    print(f\"{HOMOSCEDASTICITY_GOOD} Breusch-Pagan Test: statistic={bp_stat2:.4f}, p-value={bp_p2:.4f}\")\n",
    "    print(\"Interpretation of Breusch-Pagan Test:\")\n",
    "    print(\" - p > 0.05 suggests homoscedasticity (constant variance)\")\n",
    "\n",
    "    # ---  Check for Independence of Residuals ---\n",
    "    print(\"\\n--- ASSUMPTION 3: Checking Independence of Residuals (Autocorrelation) ---\")\n",
    "    WATSON_GOOD = \"✅\" if 1.5 < durbin_watson(model.resid) < 2.5 else \"❌\"\n",
    "    durbin_watson_stat = sm.stats.durbin_watson(model.resid)\n",
    "    print(f\"{WATSON_GOOD} Durbin-Watson statistic: {durbin_watson_stat:.2f}\")\n",
    "    print(\"Interpretation: The statistic ranges from 0 to 4.\")\n",
    "    print(\"- A value around 2.0 suggests no autocorrelation.\")\n",
    "    print(\"- Values < 1.5 suggest positive autocorrelation.\")\n",
    "    print(\"- Values > 2.5 suggest negative autocorrelation.\")\n",
    "\n",
    "    # --- Check for Normality of Residuals ---\n",
    "    print(\"\\n--- ASSUMPTION 4: Checking Normality of Residuals ---\")\n",
    "    shapiro_stat2, shapiro_p2 = stats.shapiro(model.resid)\n",
    "    SHAPIRO_GOOD = \"✅\" if shapiro_p2 > 0.05 else \"❌\"\n",
    "    print(f\"{SHAPIRO_GOOD} Shapiro-Wilk Test: statistic={shapiro_stat2:.4f}, p-value={shapiro_p2:.4f}\")\n",
    "    print(\"\\nInterpretation of Shapiro-Wilk Test:\")\n",
    "    print(\" - p > 0.05 suggests residuals are normally distributed\")\n",
    "\n",
    "    # plot these side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(residuals, kde=True, ax=axs[0])\n",
    "    axs[0].set_title('Histogram of Residuals')\n",
    "    axs[0].set_xlabel('Residual')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Q-Q Plot\n",
    "    sm.qqplot(residuals, line='45', fit=True, ax=axs[1])\n",
    "    axs[1].set_title('Q-Q Plot of Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"1. Histogram: Should look like a bell curve.\")\n",
    "    print(\"2. Q-Q Plot: Points should fall along the 45-degree line.\")\n",
    "\n",
    "\n",
    "    # --- Check for Multicollinearity ---\n",
    "    print(\"\\n--- ASSUMPTION 5: Checking for Multicollinearity ---\")\n",
    "\n",
    "    # Create a DataFrame for VIF\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    # We ignore the VIF for the constant, as it's not a predictor.\n",
    "    for row, i in vif_data[1:].iterrows():\n",
    "        VIF_GOOD = \"✅\" if i['VIF'] < 5 else \"❌\"\n",
    "        print(f\"{VIF_GOOD} VIF for {i['feature']}: {i['VIF']:.4f}\")\n",
    "    print(\"\\nInterpretation of VIF:\")\n",
    "    print(\"- VIF = 1: No correlation\")\n",
    "    print(\"- 1 < VIF < 5: Moderate correlation, generally acceptable.\")\n",
    "    print(\"- VIF > 5 or 10: High correlation, indicating a multicollinearity problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labelmaps():\n",
    "    # Let's build nice Label Maps\n",
    "    labelmaps = {}\n",
    "    labelmaps['control_treatment_asnumber'] = \"1= Treatment Group\"\n",
    "    labelmaps['first_prog_course_asnumber'] = \"1= First Programming Course\"\n",
    "    labelmaps['gender_asnumber'] = \"1= Male\"\n",
    "    labelmaps['year_in_school_simplified_asnumber'] = \"1= Not First Year\"\n",
    "    labelmaps['learning_session_count'] = \"Learning Session Count\"\n",
    "    labelmaps['task_completion_session_count'] = \"Task Completion Session Count\"\n",
    "    labelmaps['total_interaction_count'] = \"Total Interactions\"\n",
    "    labelmaps['total_session_count'] = \"Total Sessions\"\n",
    "    return labelmaps\n",
    "\n",
    "def forest_plot_regression(data, labelmaps: dict = {}):\n",
    "    \"\"\"\n",
    "    Create a forest plot from a statsmodels regression result object.\n",
    "\n",
    "    Parameters:\n",
    "    data (statsmodels.regression.linear_model.RegressionResultsWrapper): The regression result object.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the forest plot.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a statsmodels regression result object\n",
    "    if not isinstance(data, sm.regression.linear_model.RegressionResultsWrapper):\n",
    "        raise ValueError(\"Input must be a statsmodels regression result object.\")\n",
    "\n",
    "\n",
    "    # Create a dataframe with independent variable statistics from the regression model\n",
    "    independent_vars_stats = pd.DataFrame({\n",
    "        'Variable': [i for i in data.params.index[1:]],\n",
    "        'Coefficient': data.params.values[1:],\n",
    "        'Std_Error': data.bse.values[1:],\n",
    "        't_statistic': data.tvalues.values[1:],\n",
    "        'p_value': data.pvalues.values[1:],\n",
    "        'CI_Lower': [data.conf_int().loc[i, 0] for i in data.params.index[1:]],\n",
    "        'CI_Upper': [data.conf_int().loc[i, 1] for i in data.params.index[1:]],\n",
    "        'N': [int(data.nobs) for _ in data.params.index[1:]],\n",
    "        'Label': [labelmaps.get(i, i) if labelmaps else i for i in data.params.index[1:]],\n",
    "        'Group': [ 'AI Interaction' if l in [\"learning_session_count\", \"task_completion_session_count\"] else 'Other' for l in data.params.index[1:]],  # No grouping in this case\n",
    "    })\n",
    "\n",
    "    return fp.forestplot(independent_vars_stats,  # the dataframe with results data\n",
    "                estimate=\"Coefficient\",  # col containing estimated effect size \n",
    "                ll=\"CI_Lower\", hl=\"CI_Upper\",  # columns containing conf. int. lower and higher limits\n",
    "                varlabel=\"Label\",  # column containing variable label\n",
    "                groupvar=\"Group\", # column containing group label (if any)\n",
    "                ylabel=f\"Confidence intervals for dependent variable {data.model.endog_names}\",  # y-label title\n",
    "                xlabel=\"Coefficient\",  # x-label title\n",
    "                pval=\"p_value\",  # column containing p-values\n",
    "                annote=[\"N\",'t_statistic', \"est_ci\"],  # additional columns to display in the plot\n",
    "                annoteheaders=[\"N\", \"T-Statistic\", \"Est. (95% Conf. Int.)\"], \n",
    "                color_alt_rows=len(data.params.index[1:]) > 2,  # Gray alternate rows,\n",
    "                table=True,\n",
    "                )\n",
    "    \n",
    "\n",
    "def linear_regression_analysis(df, independent_var_col, dependent_var_col, do_print=True, do_plot=True, robust_std_errors=False):\n",
    "    # 1. threshhold, based on apriori power analysis N=87\n",
    "    if isinstance(independent_var_col, str):\n",
    "        k_regressors = 1\n",
    "    else:\n",
    "        k_regressors = len(independent_var_col)\n",
    "    N = df.shape[0]\n",
    "    result = calculate_target_cohens_f(N, k_regressors)\n",
    "    TARGET_F = result['cohens_f']\n",
    "    TARGET_F2 = TARGET_F**2\n",
    "    TARGET_R2 = TARGET_F2 / (1 + TARGET_F2) \n",
    "\n",
    "    # 2. Define the independent (X) and dependent (y) variables\n",
    "    # We add a constant to the independent variable to fit the intercept (beta_0)\n",
    "    # of the regression model.\n",
    "    X = sm.add_constant(df[independent_var_col])  # Independent variable\n",
    "    y = df[dependent_var_col]\n",
    "\n",
    "    # 3. Fit the regression model, calculate the other vars \n",
    "    if robust_std_errors:\n",
    "        print(\"\\nFitting model with robust standard errors (HC3)...\")\n",
    "        model = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "    else:\n",
    "        print(\"\\nFitting model with standard OLS...\")\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "    PGOOD = \"✅\" if model.f_pvalue < 0.05 else \"❌\"\n",
    "    RGOOD = \"✅\" if model.rsquared >= TARGET_R2 else \"❌\"\n",
    "    cohens_f2 = model.rsquared / (1 - model.rsquared)\n",
    "\n",
    "    # 4. Print the regression summary\n",
    "    if do_print:\n",
    "        print(\"*\" * 80)\n",
    "        print(\"INDEPENDENT VAR:\", independent_var_col) \n",
    "        print(\"DEPENDENT VAR  :\", dependent_var_col)\n",
    "        print(\"REGRESSORS     :\", k_regressors)\n",
    "        print(model.summary())\n",
    "        print(\"Statistical Significance and Effect Size Checks:\")\n",
    "        print(f\"{PGOOD} P-value : {model.f_pvalue:.4f} (Threshold: <= 0.05)\")\n",
    "        print(f\"{RGOOD} R-squared: {model.rsquared:.4f} (Target R-Squared: >= {TARGET_R2:.4f})\")\n",
    "        print(f\"   Cohen's f-squared: {cohens_f2:.4f} (Cohen's target f-squared: >= {TARGET_F2:.4f})\")\n",
    "        print(f\"   Cohen's f: {np.sqrt(cohens_f2):.4f} (Cohen's target f: >= {TARGET_F:.4f})\")\n",
    "\n",
    "    if do_plot:\n",
    "        display(forest_plot_regression(model, labelmaps=build_labelmaps()))\n",
    "\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddfb3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_d_to_f(cohen_d: float) -> float:\n",
    "    \"\"\"\n",
    "    Converts Cohen's d to Cohen's f for a two-group comparison.\n",
    "\n",
    "    Cohen's d is an effect size used to indicate the standardized\n",
    "    difference between two means. Cohen's f is an effect size used in the\n",
    "    context of ANOVA. For the specific case of two groups (which is the\n",
    "    context for Cohen's d), there is a direct conversion formula.\n",
    "\n",
    "    The formula is: f = d / 2.\n",
    "\n",
    "    This function takes the absolute value of d, as effect sizes are\n",
    "    conventionally reported as non-negative values representing the\n",
    "    magnitude of an effect.\n",
    "\n",
    "    Args:\n",
    "        cohen_d (float): The value of Cohen's d.\n",
    "\n",
    "    Returns:\n",
    "        float: The corresponding value of Cohen's f.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the input cohen_d is not a numeric value.\n",
    "    \"\"\"\n",
    "    if not isinstance(cohen_d, numbers.Number):\n",
    "        raise TypeError(\"Input 'cohen_d' must be a numeric value (int or float).\")\n",
    "\n",
    "    # The conversion formula for two groups is f = d / 2.\n",
    "    # Effect sizes are conventionally positive, so we take the absolute value.\n",
    "    cohen_f = abs(cohen_d) / 2\n",
    "\n",
    "    return cohen_f\n",
    "\n",
    "def interpret_effect_size_cohens( effect_size: float) -> str:\n",
    "    \"\"\"\n",
    "    Interpret the effect size based on power analysis thresholds.\n",
    "\n",
    "    Args:\n",
    "        effect_size (float): The calculated eta-squared value.\n",
    "        \n",
    "    Returns:\n",
    "        str: Interpretation of the effect size.\n",
    "    \"\"\"\n",
    "    if np.isnan(effect_size):\n",
    "        return \"Effect size is not calculable.\"\n",
    "    \n",
    "    effect_size = abs(effect_size)\n",
    "    if effect_size < .50:\n",
    "        interpretation  = \"insignificant 👎\"\n",
    "    elif effect_size < 0.65:\n",
    "        interpretation  = \"in target range 🎯\"\n",
    "    else:\n",
    "        interpretation  = \"satisfactory 🚀\"\n",
    "\n",
    "    return interpretation \n",
    "\n",
    "def interpret_cohens_d(d):\n",
    "    \"\"\"Interprets the magnitude of Cohen's d.\"\"\"\n",
    "    d = abs(d)\n",
    "    if d < 0.2:\n",
    "        return \"Very Small\"\n",
    "    elif d < 0.5:\n",
    "        return \"Small\"\n",
    "    elif d < 0.8:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "def interpret_rank_biserial(r):\n",
    "    \"\"\"Interprets the magnitude of Rank-Biserial Correlation.\"\"\"\n",
    "    r = abs(r)\n",
    "    if r < 0.1:\n",
    "        return \"Very Small\"\n",
    "    elif r < 0.3:\n",
    "        return \"Small\"\n",
    "    elif r < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "def independent_ttest(df: pd.DataFrame, independent_var_df_col: str, dependent_var_df_col: str):\n",
    "    '''\n",
    "    Perform an independent samples t-test (A/B test) on the specified dependent variable \n",
    "    grouped by the independent variable. The independent variable must have exactly two categories.\n",
    "\n",
    "    Independent variable is the cause, should be categorical (e.g., 'Group A', 'Group B').\n",
    "    Dependent variable is the effect, should be numerical (e.g., 'Score', 'Conversion Rate').\n",
    "    '''\n",
    "\n",
    "    # --- SETUP ---\n",
    "    # Ensure there are exactly two groups\n",
    "    groups = df[independent_var_df_col].unique()\n",
    "    if len(groups) != 2:\n",
    "        raise ValueError(f\"Independent variable '{independent_var_df_col}' must have exactly two groups for a t-test. Found: {len(groups)}\")\n",
    "    \n",
    "    # break these up into two groups\n",
    "    group1_name, group2_name = groups[0], groups[1]\n",
    "    group1_data = df[df[independent_var_df_col] == group1_name][dependent_var_df_col].dropna()\n",
    "    group2_data = df[df[independent_var_df_col] == group2_name][dependent_var_df_col].dropna()\n",
    "\n",
    "    # Titles for plots and print statements\n",
    "    dependent_var_title = dependent_var_df_col.replace('_', ' ').title()\n",
    "    independent_var_title = independent_var_df_col.replace('_', ' ').title()\n",
    "\n",
    "    # --- 1. DESCRIPTIVE STATISTICS ---\n",
    "    print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "    print(\"\\n== Variables: ==\")\n",
    "    print(f\"  → Independent Variable (cause): {independent_var_df_col} ('{independent_var_title}')\")\n",
    "    print(f\"  → Dependent Variable (effect):  {dependent_var_df_col} ('{dependent_var_title}')\")\n",
    "\n",
    "    print(\"\\n== Group Means, Standard Deviations, N-tiles: ==\")\n",
    "    desc_stats = df.groupby(independent_var_df_col)[dependent_var_df_col].describe()\n",
    "    print(desc_stats)\n",
    "\n",
    "    # --- 2. VISUALIZATIONS ---\n",
    "    print(\"\\n=== VISUALIZATIONS ===\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(f\"Analysis of '{dependent_var_title}' by '{independent_var_title}'\", fontsize=16)\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(data=df, x=independent_var_df_col, y=dependent_var_df_col, ax=axes[0], hue=independent_var_df_col)\n",
    "    axes[0].set_title(f'Distribution by Group')\n",
    "    axes[0].set_xlabel(independent_var_title)\n",
    "    axes[0].set_ylabel(dependent_var_title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Violin plot\n",
    "    sns.violinplot(data=df, x=independent_var_df_col, y=dependent_var_df_col, ax=axes[1], hue=independent_var_df_col)\n",
    "    axes[1].set_title(f'Distribution Shape by Group')\n",
    "    axes[1].set_xlabel(independent_var_title)\n",
    "    axes[1].set_ylabel(dependent_var_title)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overlapping Histogram\n",
    "    axes[2].hist(group1_data, alpha=0.7, label=group1_name, bins=15, edgecolor='black')\n",
    "    axes[2].hist(group2_data, alpha=0.7, label=group2_name, bins=15, edgecolor='black')\n",
    "    axes[2].set_title(f'Overlapping Histograms')\n",
    "    axes[2].set_xlabel(dependent_var_title)\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # --- 3. CHECK ASSUMPTIONS ---\n",
    "    print(\"\\n=== ASSUMPTION CHECKS ===\")\n",
    "\n",
    "    # A. Normality test (Shapiro-Wilk for each group)\n",
    "    print(\"\\n== Normality Tests (Shapiro-Wilk): ==\")\n",
    "    normality_results = {}\n",
    "    for name, data in [(group1_name, group1_data), (group2_name, group2_data)]:\n",
    "        stat, p_value = stats.shapiro(data)\n",
    "        normality_results[name] = p_value > 0.05\n",
    "        print(f\"  → Group '{name}': W={stat:.4f}, p={p_value:.4f}\", end=\"\")\n",
    "        if normality_results[name]:\n",
    "            print(\" → Appears normally distributed (p > 0.05) ✅\")\n",
    "        else:\n",
    "            print(\" → May NOT be normally distributed (p ≤ 0.05) ❌\")\n",
    "    \n",
    "    all_normal = all(normality_results.values())\n",
    "\n",
    "    # B. Homogeneity of variance (Levene's test)\n",
    "    print(f\"\\n== Levene's Test for Equal Variances: ==\")\n",
    "    levene_stat, levene_p = stats.levene(group1_data, group2_data)\n",
    "    homogeneity = levene_p > 0.05\n",
    "    print(f\"  → Statistic={levene_stat:.4f}, p={levene_p:.4f}\", end=\"\")\n",
    "    if homogeneity:\n",
    "        print(\" → Variances appear equal (p > 0.05) - Assumption met ✅\")\n",
    "    else:\n",
    "        print(\" → Variances may be unequal (p ≤ 0.05) - Use Welch's t-test ❌\")\n",
    "\n",
    "    # --- 4. PERFORM STATISTICAL TESTS ---\n",
    "    print(\"\\n=== STATISTICAL TESTS ===\")\n",
    "\n",
    "    # A. Test Selection\n",
    "    parametric = all_normal and homogeneity\n",
    "    print(\"\\n== Test Selection ==\")\n",
    "    print(f\"  → Normality assumption met for all groups: {all_normal}\")\n",
    "    print(f\"  → Homogeneity of variance assumption met: {homogeneity}\")\n",
    "    if parametric:\n",
    "        print(\"  → Conclusion: Assumptions for parametric test (Student's t-test) are met. ✅\")\n",
    "    else:\n",
    "        print(\"  → Conclusion: Assumptions for standard parametric test are NOT fully met. ❌\")\n",
    "        if not homogeneity:\n",
    "            print(\"    - Welch's t-test will be used due to unequal variances.\")\n",
    "        if not all_normal:\n",
    "            print(\"    - Mann-Whitney U test is the recommended non-parametric alternative.\")\n",
    "\n",
    "    # B. Perform T-Test (Student's or Welch's)\n",
    "    parametric_icon = \"✅\" if parametric else \"⚠️\"\n",
    "    test_type = \"Student's T-Test\" if homogeneity else \"Welch's T-Test\"\n",
    "    print(f\"\\n== {parametric_icon} Parametric Test: {test_type} Results ==\")\n",
    "    \n",
    "    # Use equal_var=False for Welch's t-test if variances are unequal\n",
    "    t_stat, t_p = stats.ttest_ind(group1_data, group2_data, equal_var=homogeneity)\n",
    "\n",
    "    print(f\"  → T-statistic={t_stat:.4f}, p-value={t_p:.4f}\")\n",
    "    if t_p < 0.05:\n",
    "        print(f\"  → There IS a statistically significant difference between the means of the two groups (p < 0.05) ✅\")\n",
    "    else:\n",
    "        print(f\"  → There is NOT a statistically significant difference between the means of the two groups (p ≥ 0.05) ❌\")\n",
    "\n",
    "    # C. Perform Mann-Whitney U Test (non-parametric alternative)\n",
    "    non_parametric_icon = \"✅\" if not all_normal else \"⚠️\"\n",
    "    print(f\"\\n== {non_parametric_icon} Non-Parametric Test: Mann-Whitney U Results ==\")\n",
    "    \n",
    "    u_stat, u_p = stats.mannwhitneyu(group1_data, group2_data, alternative='two-sided')\n",
    "    \n",
    "    print(f\"  → U-statistic={u_stat:.4f}, p-value={u_p:.4f}\")\n",
    "    if u_p < 0.05:\n",
    "        print(f\"  → There IS a statistically significant difference between the distributions of the two groups (p < 0.05) ✅\")\n",
    "    else:\n",
    "        print(f\"  → There is NOT a statistically significant difference between the distributions of the two groups (p ≥ 0.05) ❌\")\n",
    "\n",
    "    # --- 5. EFFECT SIZES ---\n",
    "    print(\"\\n=== EFFECT SIZES ===\")\n",
    "\n",
    "    # A. Cohen's d for T-Test\n",
    "    n1, n2 = len(group1_data), len(group2_data)\n",
    "    s1, s2 = np.var(group1_data, ddof=1), np.var(group2_data, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    mean_diff = np.mean(group1_data) - np.mean(group2_data)\n",
    "    cohens_d = mean_diff / pooled_std\n",
    "    \n",
    "    print(f\"\\n== {parametric_icon} Cohen's d (strength of difference between means) ==\")\n",
    "    print(f\"  → Effect Size (d): {cohens_d:.4f}\")\n",
    "    d_interpretation = interpret_cohens_d(cohens_d)\n",
    "    print(f\"  → Effect size interpretation: {d_interpretation.upper()}\")\n",
    "\n",
    "    # C. Interpret Cohen's d\n",
    "    print(f\"\\n== {parametric_icon} Cohen's d Interpretation based on Statistical Power 0.8 ==\")\n",
    "    print(f\"  → {interpret_effect_size_cohens(cohens_d)}\")\n",
    "\n",
    "    # B. Rank-Biserial Correlation for Mann-Whitney U\n",
    "    rank_biserial = 1 - (2 * u_stat) / (n1 * n2)\n",
    "    \n",
    "    print(f\"\\n== {non_parametric_icon} Rank-Biserial Correlation (strength of difference between distributions) ==\")\n",
    "    print(f\"  → Effect Size (r): {rank_biserial:.4f}\")\n",
    "    r_interpretation = interpret_rank_biserial(rank_biserial)\n",
    "    print(f\"  → Effect size interpretation: {r_interpretation.upper()}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e03f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_effect_size( effect_size: float) -> str:\n",
    "    \"\"\"\n",
    "    Interpret the effect size based on common thresholds.\n",
    "    Small Effect: = 0.01\n",
    "    Medium Effect: = 0.06\n",
    "    Large Effect: = 0.14\n",
    "\n",
    "    Args:\n",
    "        effect_size (float): The calculated effect size value.\n",
    "\n",
    "    Returns:\n",
    "        str: Interpretation of the effect size.\n",
    "    \"\"\"\n",
    "    if effect_size < 0.01:\n",
    "        return \"insignificant\"\n",
    "    elif effect_size < 0.06:\n",
    "        return \"small\"\n",
    "    elif effect_size < 0.14:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "    \n",
    "def calculate_eta_squared(df: pd.DataFrame, independent_var_col: str, dependent_var_col: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the eta-squared effect size for a one-way ANOVA.\n",
    "\n",
    "    Eta-squared (η²) is a measure of the proportion of variance in the dependent\n",
    "    variable that is associated with the independent variable.\n",
    "\n",
    "    Inputs:\n",
    "        df: A pandas DataFrame containing the data.\n",
    "        dependent_var_col: The name of the column for the continuous dependent variable.\n",
    "        independent_var_col: The name of the column for the categorical independent variable (groups).\n",
    "\n",
    "    Returns:\n",
    "        The eta-squared value as a float. Returns np.nan if calculations fail.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Step 1: Calculate the Overall Mean ---\n",
    "        # This is the overall mean of the dependent variable for all groups combined.\n",
    "        grand_mean = df[dependent_var_col].mean()\n",
    "\n",
    "        # --- Step 2: Calculate the Total Sum of Squares (SS_total) ---\n",
    "        # This measures the total variation in the data. It's the sum of the\n",
    "        # squared differences between each individual data point and the grand mean.\n",
    "        ss_total = ((df[dependent_var_col] - grand_mean) ** 2).sum()\n",
    "\n",
    "        # --- Step 3: Calculate the Sum of Squares Between Groups (SS_between) ---\n",
    "        # This measures the variation between the different group means and the grand mean.\n",
    "        ss_between = 0\n",
    "        # Get the unique group labels from the independent variable column\n",
    "        groups = df[independent_var_col].unique()\n",
    "        \n",
    "        for group in groups:\n",
    "            # Get the data for the current group\n",
    "            group_data = df[df[independent_var_col] == group]\n",
    "            # Calculate the mean for this specific group\n",
    "            group_mean = group_data[dependent_var_col].mean()\n",
    "            # Get the number of observations in this group\n",
    "            n_group = len(group_data)\n",
    "            # Calculate the squared difference, weighted by group size, and add to the total\n",
    "            ss_between += n_group * ((group_mean - grand_mean) ** 2)\n",
    "\n",
    "        # --- Step 4: Calculate Eta-Squared ---\n",
    "        # The formula is the ratio of the variance explained by the groups\n",
    "        # to the total variance in the data.\n",
    "        if ss_total == 0:\n",
    "            # Avoid division by zero if there is no variance in the data\n",
    "            return np.nan \n",
    "            \n",
    "        eta_squared = ss_between / ss_total\n",
    "\n",
    "        return eta_squared\n",
    "\n",
    "    except (KeyError, TypeError) as e:\n",
    "        print(f\"An error occurred: {e}. Please check your column names and data types.\")\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "def one_way_anova(df: pd.DataFrame, independent_var_df_col: str, dependent_var_df_col: str):\n",
    "    '''\n",
    "    Perform a one-way ANOVA on the specified dependent variable grouped by the independent variable.\n",
    "    The independent variable should be categorical with 2 > categories (otherwise use an independent samples t-test)\n",
    "\n",
    "    Independent variable is the cause, should be categorical\n",
    "    Dependent variable is the effect, should be numerical\n",
    "    '''\n",
    "\n",
    "    # Titles\n",
    "    dependent_var_title = dependent_var_df_col.replace('_', ' ').title()\n",
    "    independent_var_title = independent_var_df_col.replace('_', ' ').title()\n",
    "\n",
    "    # 1. DESCRIPTIVE STATISTICS\n",
    "    print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "    print(\"\\n==Variables: ==\")\n",
    "    print(f\"  → Independent Variable (cause): {independent_var_df_col} '{independent_var_title}'\")\n",
    "    print(f\"  → Dependent Variable (effect):  {dependent_var_df_col} '{dependent_var_title}'\")\n",
    "\n",
    "\n",
    "    print(\"\\n==Group Means, Standard Deviations, N-tiles: ==\")\n",
    "    desc_stats = df.groupby(independent_var_df_col)[dependent_var_df_col].describe()\n",
    "    print(desc_stats)\n",
    "\n",
    "    # 2. VISUALIZATIONS\n",
    "    print(\"\\n=== VISUALIZATIONS ===\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(data=df, x=independent_var_df_col, y=dependent_var_df_col, ax=axes[0], hue=independent_var_df_col)\n",
    "    axes[0].set_title(f'Distribution of {independent_var_title} by {dependent_var_title}')\n",
    "    axes[0].set_xlabel(dependent_var_title)\n",
    "    axes[0].set_ylabel(independent_var_title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Violin plot\n",
    "    sns.violinplot(data=df, x=independent_var_df_col, y=dependent_var_df_col, ax=axes[1], hue=independent_var_df_col)\n",
    "    axes[1].set_title(f'Distribution Shape by {dependent_var_title}')\n",
    "    axes[1].set_xlabel(dependent_var_title)\n",
    "    axes[1].set_ylabel(independent_var_title)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram\n",
    "    for category in df[independent_var_df_col].unique():\n",
    "        subset = df[df[independent_var_df_col] == category][dependent_var_df_col]\n",
    "        axes[2].hist(subset, alpha=0.7, label=category, bins=15, edgecolor='black')\n",
    "    axes[2].set_title(f'Histograms by {dependent_var_title}')\n",
    "    axes[2].set_xlabel(independent_var_title)\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].set_xlim(df[dependent_var_df_col].min(), df[dependent_var_df_col].max())\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. CHECK ASSUMPTIONS FOR ANOVA\n",
    "\n",
    "    # A. Normality test (Shapiro-Wilk for each group)\n",
    "    print(\"\\n=== ASSUMPTION CHECKS ===\")\n",
    "    print(\"\\n== Normality Tests (Shapiro-Wilk): ==\")\n",
    "    print(\"Categories:\")\n",
    "    for category in df[independent_var_df_col].unique():\n",
    "        group_data = df[df[independent_var_df_col] == category][dependent_var_df_col].dropna()\n",
    "        stat, p_value = stats.shapiro(group_data)\n",
    "        print(f\"  → {category}: W={stat:.4f}, p={p_value:.4f}\", end =\"\")\n",
    "        normality = p_value > 0.05\n",
    "        if normality:\n",
    "            print(f\"  → {category} appears normally distributed (p > 0.05) ✅\")\n",
    "        else:\n",
    "            print(f\"  → {category} may NOT be normally distributed (p ≤ 0.05) ❌\")\n",
    "\n",
    "    # B. Homogeneity of variance (Levene's test)\n",
    "    print(f\"\\n== Levene's Test for Equal Variances: ==\")\n",
    "    groups = []\n",
    "    for category in df[independent_var_df_col].unique():\n",
    "        group_data = df[df[independent_var_df_col] == category][dependent_var_df_col].dropna()\n",
    "        groups.append(group_data)\n",
    "\n",
    "    levene_stat, levene_p = stats.levene(*groups)\n",
    "    print(f\"  → Statistic={levene_stat:.4f}, p={levene_p:.4f}\", end=\"\")\n",
    "    homogeneity = levene_p > 0.05\n",
    "    if homogeneity:\n",
    "        print(\"  → Variances appear equal (p > 0.05) - ANOVA assumption met ✅\")\n",
    "    else:\n",
    "        print(\"  → Variances may be unequal (p ≤ 0.05) - Consider Welch's ANOVA ❌\")\n",
    "    print()\n",
    "\n",
    "    # 4. PERFORM STATISTICAL TESTS\n",
    "    print(\"\\n=== STATISTICAL TESTS ===\")\n",
    "\n",
    "    # A. TEST SELECTION\n",
    "    parametric = homogeneity and normality\n",
    "    parametric_icon = \"✅\" if parametric else \"❌\"\n",
    "    non_parametric_icon = \"❌\" if parametric else \"✅\"\n",
    "    print(\"\\n== Test Selection ==\")\n",
    "    print(f\"  → parametric={parametric}, homogenity={homogeneity} → \", end=\"\")\n",
    "    if parametric:\n",
    "        print(f\" Assumptions for parametric tests (ANOVA) are met. ✅\")\n",
    "    else:\n",
    "        print(\"Assumptions for parametric tests (ANOVA) are NOT met. ❌\")\n",
    "\n",
    "\n",
    "    # A. PERFORM ONE-WAY ANOVA\n",
    "    print(f\"\\n== {parametric_icon} Parametric Test: One-Way ANOVA Results ==\")\n",
    "\n",
    "    # One way anova, compares the means of groups\n",
    "    f_stat, anova_p = stats.f_oneway(*groups)\n",
    "\n",
    "    print(f\"  → F-statistic={f_stat:.4f}, p={anova_p:.4f}\", end=\"\")\n",
    "\n",
    "    if anova_p < 0.05:\n",
    "        print(f\"  → Independent variable '{independent_var_title}' HAS a statistically significant effect on the dependent variable '{dependent_var_title}' (p < 0.05) ✅\")\n",
    "        print(\"   → Post-hoc tests needed to determine which groups differ\")\n",
    "        posthoc = True \n",
    "    else:\n",
    "        print(f\"  → Independent variable '{independent_var_title}' DOES NOT HAVE a statistically significant effect on the dependent variable '{dependent_var_title}' (p ≥ 0.05) ❌\")\n",
    "        posthoc = False\n",
    "\n",
    "\n",
    "    print(f\"\\n== {non_parametric_icon} Non-Parametrics: Kruskal-Wallis Results ==\")\n",
    "\n",
    "    # Kruskal-Wallis test (non-parametric alternative) compares the median ranks of groups\n",
    "    kw_stat, kw_p = stats.kruskal(*groups)\n",
    "    \n",
    "    print(f\"  → H-statistic={kw_stat:.4f}, p={kw_p:.4f}\", end=\"\")\n",
    "\n",
    "    if kw_p < 0.05:\n",
    "        print(f\"  → Independent variable '{independent_var_title}' HAS a statistically significant effect on the dependent variable '{dependent_var_title}' (p < 0.05) ✅\")\n",
    "        print(\"   → Post-hoc tests needed to determine which groups differ\")\n",
    "        posthoc = True \n",
    "    else:\n",
    "        print(f\"  → Independent variable '{independent_var_title}' DOES NOT HAVE a statistically significant effect on the dependent variable '{dependent_var_title}' (p ≥ 0.05) ❌\")\n",
    "        posthoc = False\n",
    "\n",
    "    # 5. EFFECT SIZE (Eta-squared)\n",
    "\n",
    "  \n",
    "    print(\"\\n=== EFFECT SIZES ===\")\n",
    "\n",
    "    # A. Calculate eta-squared (proportion of variance explained)\n",
    "    eta_squared = calculate_eta_squared(df, independent_var_df_col, dependent_var_df_col)\n",
    "    print(f\"\\n== {parametric_icon} Eta-Squared (η²) strength of relationship (ANOVA) ==\")\n",
    "    print(f\"  → Effect Size (η²): {eta_squared:.4f}\", end=\"\")\n",
    "    eta_squared_effect_size = interpret_effect_size(eta_squared)\n",
    "    print(f\" → Effect size interpretation: {eta_squared_effect_size.upper()}\")\n",
    "    print(f\"    → Approximately {eta_squared * 100:.2f}% of the variance in '{dependent_var_title}' can be explained by '{independent_var_title}'.\")\n",
    "\n",
    "\n",
    "    # B. Calculate Epsilon Squared\n",
    "    epsilon_squared = kw_stat / (len(df) - 1)\n",
    "    print(f\"\\n== {non_parametric_icon} Epsilon-Squared (ε²) proportion of variance (Kruskal-Wallis) ==\")\n",
    "    print(f\"  → Effect Size (ε²): {epsilon_squared:.4f}\", end=\"\")\n",
    "    epsilon_squared_effect_size = interpret_effect_size(eta_squared)\n",
    "    print(f\" → Effect size interpretation: {epsilon_squared_effect_size.upper()}\")\n",
    "    print(f\"    → Approximately {epsilon_squared * 100:.2f}% of the variance in '{dependent_var_title}' can be explained by '{independent_var_title}'.\")\n",
    "\n",
    "\n",
    "    # C. Convert to Cohen's F  (for >= 3 groups)\n",
    "    if parametric:\n",
    "        source_squared = eta_squared\n",
    "    else:\n",
    "        source_squared = epsilon_squared\n",
    "\n",
    "    cohens_f_squared = source_squared / (1 - source_squared)\n",
    "    cohens_f = np.sqrt(cohens_f_squared)\n",
    "    print(f\"\\n== {parametric_icon} Cohen's F (f²) effect size ==\")\n",
    "    print(f\"  → Effect Size (f²): {cohens_f:.4f}\", end=\"\")\n",
    "    cohens_f_effect_size = interpret_effect_size_cohens(cohens_f)\n",
    "    print(f\" → Effect size interpretation with respect to power: {cohens_f_effect_size.upper()}\")\n",
    "    print(f\"    → Approximately {cohens_f * 100:.2f}% of the variance in '{dependent_var_title}' can be explained by '{independent_var_title}'.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c454ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e18f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
