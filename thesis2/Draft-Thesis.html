<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>📚 Draft-Thesis</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title"><span id="_b592xwmyaqv9" class="anchor"></span>📚 Draft-Thesis</h1>
</header>
<h1 id="contents"><strong>Contents</strong></h1>
<p><a href="#contents"><strong>Contents 1</strong></a></p>
<p><a href="#introduction"><strong>1.0 Introduction 3</strong></a></p>
<p><a href="#literature-review"><strong>2.0 Literature Review 5</strong></a></p>
<blockquote>
<p><a href="#rise-of-the-large-language-model-llm">2.1 Rise of the Large Language Model (LLM) 6</a></p>
<p><a href="#llms-as-a-disruptive-innovation-for-software-development">2.2 LLMs as a Disruptive Innovation for software development 7</a></p>
</blockquote>
<p><a href="#methods"><strong>3.0 Methods 9</strong></a></p>
<blockquote>
<p><a href="#introduction-1">3.1 Introduction 9</a></p>
<p><a href="#research-questions">3.1.1 Research Questions 9</a></p>
<p><a href="#study-design">3.2 Study Design 10</a></p>
<p><a href="#overview">3.2.1 Overview 11</a></p>
<p><a href="#participant-funnel">3.2.2 Participant Funnel 14</a></p>
<p><a href="#population-1-chatbot-participants">3.2.2.1 Population 1: Chatbot participants 15</a></p>
<p><a href="#population-2-chatbot-participants-with-survey-responses">3.2.2.1 Population 2: Chatbot participants with Survey Responses 16</a></p>
<p><a href="#computational-literacy-instrument-c1c2">3.2.3 Computational Literacy Instrument (C1/C2) 16</a></p>
<p><a href="#chatbot-design">3.2.4 Chatbot Design 18</a></p>
<p><a href="#llm-selection">3.2.4.1 LLM Selection 18</a></p>
<p><a href="#random-assignment">3.2.4.2 Random Assignment 19</a></p>
<p><a href="#control-group-t1">3.2.4.3 Control Group (T1) 19</a></p>
<p><a href="#treatment-group-t2">3.2.4.4 Treatment Group (T2) 20</a></p>
<p><a href="#midterm-exam-e1">3.2.5 Midterm Exam (E1) 21</a></p>
<p><a href="#questionnaire-q1">3.2.6 Questionnaire (Q1) 23</a></p>
<p><a href="#chatbot-trace-data-d1">3.2.7 Chatbot Trace Data (D1) 23</a></p>
<p><a href="#data-analysis">3.3 Data Analysis 25</a></p>
<p><a href="#overview-1">3.3.1 Overview 25</a></p>
<p><a href="#tools">3.3.2 Tools 26</a></p>
<p><a href="#operationalizing-d1-chatbot-trace-data">3.3.3 Operationalizing D1 Chatbot Trace Data 26</a></p>
<p><a href="#model-selection-reliability-testing">3.3.4 Model Selection Reliability Testing 29</a></p>
<p><a href="#categorical-content-analysis">3.3.5 Categorical Content Analysis 31</a></p>
<p><a href="#hypothesis-formulation-and-methodology">3.4 Hypothesis Formulation and Methodology 32</a></p>
<p><a href="#rq1-hypothesis-and-methodology">3.4.1 RQ1 Hypothesis and Methodology 32</a></p>
<p><a href="#rq2-hypothesis-and-methodology">3.4.2 RQ2 Hypothesis and Methodology 34</a></p>
<p><a href="#rq3-hypothesis-and-methodology">3.4.3 RQ3 Hypothesis and Methodology 36</a></p>
<p><a href="#satisfying-assumptions-of-linear-regressions">3.4.4 Satisfying Assumptions of Linear Regressions 38</a></p>
</blockquote>
<p><a href="#results"><strong>4.0 Results 39</strong></a></p>
<blockquote>
<p><a href="#introduction-2">4.1 Introduction 39</a></p>
<p><a href="#findings-for-rq1">4.2 Findings for RQ1 39</a></p>
<p><a href="#model-overview-for-rq1">4.2.1 Model Overview for RQ1 39</a></p>
<p><a href="#regression-assumption-tests-of-the-model-for-rq1">4.2.2 Regression Assumption Tests of the Model for RQ1 40</a></p>
<p><a href="#hierarchical-regression-analysis-for-the-rq1-model">4.2.3 Hierarchical Regression Analysis for the RQ1 Model 43</a></p>
<p><a href="#model-1-e1-task-complection-session-count">4.2.3.1 Model 1: E1 ~ Task Complection Session Count 44</a></p>
<p><a href="#model-2-e1-learning-session-count">4.2.3.2 Model 2: E1 ~ Learning Session Count 45</a></p>
<p><a href="#model-3-e1-learning-session-count-task-completion-session-count">4.2.3.3 Model 3: E1 ~ Learning Session Count + Task Completion Session Count 46</a></p>
<p><a href="#summary-of-findings-for-rq1">4.2.4 Summary of Findings for RQ1 47</a></p>
<p><a href="#conclusion-for-rq1">4.2.5 Conclusion for RQ1 49</a></p>
<p><a href="#findings-for-rq2">4.3 Findings for RQ2 49</a></p>
<p><a href="#model-overview-for-rq2">4.3.1 Model Overview for RQ2 49</a></p>
<p><a href="#regression-assumption-tests-for-the-rq2-model">4.3.2 Regression Assumption Tests for the RQ2 Model 50</a></p>
<p><a href="#regression-analysis-for-the-rq2">4.3.3 Regression Analysis for the RQ2 52</a></p>
<p><a href="#multiple-regression-with-usage-as-covariates-for-rq2">4.3.3.1 Multiple Regression with usage as Covariates for RQ2 54</a></p>
<p><a href="#mediation-analysis-for-rq2">4.3.3.2 Mediation Analysis for RQ2 55</a></p>
<p><a href="#moderation-analysis-for-rq2">4.3.3.3 Moderation Analysis for RQ2 56</a></p>
<p><a href="#summary-of-findings-for-rq2">4.3.4 Summary of Findings for RQ2 58</a></p>
<p><a href="#conclusion-for-rq2">4.3.5 Conclusion for RQ2 59</a></p>
<p><a href="#findings-for-rq3">4.4 Findings for RQ3 60</a></p>
<p><a href="#model-overview-for-rq3">4.4.1 Model Overview for RQ3 60</a></p>
<p><a href="#regression-assumption-tests-for-the-rq3-model">4.4.2 Regression Assumption Tests for the RQ3 Model 61</a></p>
<p><a href="#regression-analysis-for-rq3-accounting-for-test-retest">4.4.3 Regression Analysis for RQ3 Accounting for test-retest 65</a></p>
<p><a href="#supplementary-analysis-change-in-scores">4.4.3.1 Supplementary Analysis Change in Scores 67</a></p>
<p><a href="#summary-of-findings-for-rq3">4.4.4 Summary of Findings for RQ3 68</a></p>
<p><a href="#conclusion-for-rq3">4.4.5 Conclusion for RQ3 69</a></p>
<p><a href="#overall-summary-of-findings">4.5 Overall Summary of Findings 70</a></p>
</blockquote>
<p><a href="#summary"><strong>5.0 Summary 70</strong></a></p>
<p><a href="#references"><strong>References 71</strong></a></p>
<h1 id="section"><br />
</h1>
<h1 id="introduction"><strong>1.0 Introduction</strong></h1>
<p>NOTE: This is a work in progress….</p>
<p>This thesis investigates how the use of the Large Language Models (LLM) impact student learning performance and computational literacy. Participants were recruited from an introductory Python programming course to participate in the empirical study.. As a long time instructor of the course, previous observations revealed students use AI for both superficial task completion and to gain a deeper understanding of course material. To study the repercussions of this phenomena, student interactions with a course-provided LLM were captured, categorized then quantified to determine their correlation with academic outcomes. Given the disruptive potential of LLMs in software development and education, the findings will provide crucial insights into how to best leverage LLMs to foster positive learning outcomes and mitigate negative ones for novice programmers.</p>
<p>This study was very much a realization from years of teaching introductory Python programming to undergraduates. CLEAN THIS UP….into which facets of Large Language model use influence student grades. Oftentimes students are outcomes-focused so any insights into LLM use that foster positive outcomes, yet diminish negative ones would likely serve as a motivational driver.</p>
<p>Having taught undergraduates Python programming to undergraduates for seven years, I’ve observed several trends (ELABORATE). I am constantly improving the course and its delivery to help students achieve those outcomes. When generative AI burst onto the scene it was a disruptor. Students now had a programming expert at their fingertips. It could serve as a 24/7 tutor, teacher, code debugger. It was an entity students could now use to have an dialog on-demand for their questions. I embraced it not only allowing use in my courses but encouraging it.</p>
<p>With all of the upside, the downside of course, it can be used to circumvent learning though not-so constructive activities such as generating answers to assignments, or having AI explain their own code as part of a code reflection.</p>
<p>My suspicion was that this type of superficial learning was going on in my courses. My observation was based on students with quality assignments, yet poor exam scores. There are a variety of reasons why student achievement on the numerous practice activities in the course might not correlate with an exam score. And I’ve always had this problem to some degree, my sense was AI was exacerbating the problem.</p>
<p>While I lacked the evidence to support it, on rare occasions I would observe it. For example, students would come to office hours stuck on a problem. I’d ask them to walk through what they had so far, but they could not explain it. They informed me AI helped them with the problem but their understanding of the generated code was so lacking they could not adapt it to fit the problem.</p>
<p>This got me thinking. Perhaps I should observe students completing a programming task with AI and ask them to think-aloud while they do it. I invited a handful of students to participate and observed the results. Of the behaviors observed, two were the most interesting.</p>
<p>Some students engaged in scaffolding with the AI, using it to support and justify their own thinking as they solved the problem in code. These students were critical of the code that was generated, seeking to understand it and often asking the AI to explain or justify its decisions.</p>
<p>Another group of students used the AI for task completion; delegating to it and trusting the generations were correct. There was clear evidence of overreliance, with students believing the problem was solved correctly, when it was not. The students that were able to identify the execution output from the AI-generated was not correct, would often start over or make unreasonable requests for the AI to fix itself.</p>
<h1 id="literature-review"><strong>2.0 Literature Review</strong></h1>
<p>The following literature review presents the necessary background to better understand Large Language Models (LLMs) and their impacts on how novice programmers learn to code. We begin by exploring key research around LLMs and how they have impacted disciplines since their inception. Specifically, we provide evidence of LLMs as a disruptive innovation in the field of computer programming. LLMs have transformed not only how people code, but the ways in which programmers think about programming itself. This has implications for not only how code will be written in the future, but what skills are necessary for becoming a programmer and the methods by which those skills are taught to novices.</p>
<p>In addition, this literature review will explore the research of LLM use by teachers, students and administration within higher education. The research embodying the strengths, drawbacks, and unique challenges of LLMs within this context is discussed. This is necessary to differentiate between the overarching issues of LLMs in education and those specific to the topic of computer programming education.</p>
<p>Research will show LLMs are transforming how skilled professionals write code. LLMs are being used predominantly by teachers and students in computer programming education as well, which has forced academics to rethink not only how to teach computer programming to novices, but what should be taught in the first place. The literature will identify this through the positive and negative impacts LLMs are having programming education.</p>
<p>The academic literature on computer science education discusses the well-known challenges of novice users learning to program. These challenges are the impacts of cognitive load on learning, getting learners to focus on computational literacy over syntax and technology, the role of self-efficacy and motivation on success, and the importance of metacognition for building problem-solving skills. This literature review will revisit each of these challenges through the lens of how LLMs are impacting them both positively and negatively.</p>
<h3 id="rise-of-the-large-language-model-llm"><strong>2.1 Rise of the Large Language Model (LLM)</strong></h3>
<p>Natural Language Processing (NLP) has undergone significant changes in the past few years. With the introduction of transformer-based neural network architecture, Natural Language Processing took a giant step forward in performance and accuracy <a href="https://www.zotero.org/google-docs/?8jjxZA">(Gillioz et al., 2020)</a>.</p>
<p>The Generative Pre-Trained Transformer (GPT) reasonably predicts the next word in a sequence using the input and previously generated output. While predicting the next token in the sequence is the extent of their ability <a href="https://www.zotero.org/google-docs/?e7ZTju">(Shanahan, 2024)</a>, transformer-based language models trained on large data have demonstrated highly effective reasoning capabilities.</p>
<p>Open AI's foundational paper, "Language Models are Few-Shot Learners", demonstrated these transformer-based language models can produce human-level performance when they are trained on large copra <a href="https://www.zotero.org/google-docs/?9CR2N9">(Brown et al., 2020)</a>. This was a pivotal discovery because at the time since prior to this paper transformer-based models were trained to be relatively task specific <a href="https://www.zotero.org/google-docs/?Fu23s5">(Zhao et al., 2023)</a>.</p>
<p>The paper from <a href="https://www.zotero.org/google-docs/?cccr3c">Brown et al., 2020</a> led to significant advancements in research with respect to understanding the capabilities of LLMs. <a href="https://www.zotero.org/google-docs/?BqrLlH">Wei et al. (2022)</a> discovered reasoning capabilities of LLM's can be improved through a technique called chain-of-thought prompting. By including few-shot examples that break down complex reasoning into steps, the LLM can use those shots provided as an example of how to explain a complex process.</p>
<p><a href="https://www.zotero.org/google-docs/?sIUJsR">(Halevy et al., 2009)</a> seminal paper, “The unreasonable effectiveness of data”, explains as the training data set size increases, so does the model accuracy. In addition, the specific selected model algorithm becomes less relevant as training data set size increases. <a href="https://www.zotero.org/google-docs/?gEbEyt">(Wei, Bosma, et al., 2022)</a> documented a similar effect with large language models. Larger-sized models exhibited emergent abilities not found in their smaller counterparts. Examples of emergent abilities seen in the larger models include complex arithmetic and reading comprehension.</p>
<h3 id="llms-as-a-disruptive-innovation-for-software-development"><strong>2.2 LLMs as a Disruptive Innovation for software development</strong></h3>
<p><a href="https://www.zotero.org/google-docs/?xFDXCJ">(Christensen et al., 2018)</a> divide technological innovations into two distinct types. Sustaining innovations improve existing products and services, while disruptive innovations provide a unique set of features to an initial set of customers. From the perspective of companies that offer generative AI such as Google, Anthropic, Open AI, and Microsoft, Horn considers generative AI to be a sustaining innovation <a href="https://www.zotero.org/google-docs/?xCIUps">(Horn, 2024)</a>. In their comprehensive literature review of AI as a disruptive innovation, <a href="https://www.zotero.org/google-docs/?45LH9g">(Păvăloaia &amp; Necula, 2023)</a> consider the application of Generative AI to be a disruptive innovation across different sectors such as healthcare, agriculture business and education.</p>
<p>The rise of AI-assistant programming tools in industry is evidence of this disruption. There are a growing set of tools available in the cloud: Github Copilot, Amazon CodeWhisperer, Gemini Code assist, Claude Code, Open AI Codex v2, Tabnine, and Codeium, in addition to self-hosted options like FauxPilot, Tabby, and CodeLLama. While each provides a unique set of features for differentiation, they all have primary functions like code completion, code generation, code explanations, and discussion. The primary value-add touted by these tools is developers will be able to write code in less time, improving productivity. Talk about “vibe coding” and Lovable, Bolt, Replit and Cursor.</p>
<p>TODO: Complete the literature review….</p>
<p>IMPORTANT: The gap in the literature is tying the AI use to the grade! Point to studies showing how students use AI and that it may be helpful or harmful. Also find studies on superficial learning and impacts on grades ( just learning in general).</p>
<p>Other learning strategies are strategic like gaining a deeper understanding.</p>
<h1 id="methods"><strong>3.0 Methods</strong></h1>
<h2 id="introduction-1">3.1 Introduction</h2>
<p>The objective of this research is to study the impact of generative AI on an individual's learning. Specifically, this research studied the impacts of the Large-Language Model (LLM) used by students enrolled in an introductory Python programming course. Their learning was studied through the lenses of academic performance on a summative assessment at the mid-term of the course in addition to scores on a computational literacy instrument. Trace data from the use of LLMs was systematically categorized into activities that demonstrate superficial task completion versus those where the participant seeks to learn. These AI interactions were then quantified for each participant so that the final unit of analysis could investigate impact on exam scores. To further understand the complexities of AI use, a control and treatment group established as part of a randomized controlled study, to discover what influence, if any at all, context-awareness had on exam scores. This chapter explains the methodology that was used to address the research questions of this study.</p>
<h3 id="research-questions">3.1.1 Research Questions</h3>
<p>For this study, the following research questions were established. These questions were addressed using a mixed-methods approach consisting of content analysis <a href="https://www.zotero.org/google-docs/?uG3pUt">(Krippendorff, 2018)</a> of participant interactions with LLMs and statistical analysis <a href="https://www.zotero.org/google-docs/?ZTlUYY">(Creswell &amp; Creswell, 2017: 60)</a>.</p>
<p><em><strong>RQ1: How does large-language model use influence student learning performance?</strong></em></p>
<p>This question was addressed using a quantitative correlational design <a href="https://www.zotero.org/google-docs/?xdLUt4">(Creswell &amp; Creswell, 2017: ch8)</a> where the content analysis from participant’s LLM usage was the independent variable and their exam was the dependent variable. Hierarchical linear regression analysis <a href="https://www.zotero.org/google-docs/?yQ82m3">(Raudenbush &amp; Bryk, 2002)</a> was used to explain the contribution of each of the independent variables in addition to their collective impact on exam scores.</p>
<p><em><strong>RQ2: When the prompt is adjusted to include assignment instructions (in-context learning) what is the impact on student learning performance?</strong></em></p>
<p>This question was addressed using a between-subjects randomized controlled trial <a href="https://www.zotero.org/google-docs/?hW9Uri">(Creswell &amp; Creswell, 2017: ch8)</a>. Linear regression analysis was used to evaluate the causal effect of in-context learning on student exam performance as a dummy variable and exam score as the dependent variable,. There were follow-up mediation and moderation analyses <a href="https://www.zotero.org/google-docs/?Z6x4PU">(Edwards &amp; Lambert, 2007)</a> to explore underlying mechanisms of LLM usage.</p>
<p><em><strong>RQ3: What is the relationship between large language model use and computational literacy?</strong></em></p>
<p>This question was addressed using quantitative correlational design with repeated measures <a href="https://www.zotero.org/google-docs/?KiDAww">(Creswell &amp; Creswell, 2017: ch8)</a>. Participants' computational literacy was measured at separate intervals (beginning of the course and a midterm) using identical instruments. Similar to RQ1 linear regression analysis was used where the content analysis from participant’s LLM usage was the independent variable and their exam was the dependent variable. ANCOVA was used to account for individual differences in prior knowledge. To provide a comprehensive understanding of LLM usage computational literacy relationships, a supplementary change score analysis examined whether usage patterns predicted improvement magnitude.</p>
<h2 id="study-design">3.2 Study Design</h2>
<p><img src="./publish/thesis2/html/media/image19.png" style="width:6.5in;height:3.29167in" alt="A diagram of the study design" /><br />
<em>Figure1: An overview of the study design.</em></p>
<h3 id="overview">3.2.1 Overview</h3>
<p>The study took place over a six-week period in the Spring 2025 semester of an introductory Python programming course, IST256 (<a href="https://ist256.com/spring2025/syllabus/"><span class="underline">https://ist256.com/spring2025/syllabus/</span></a>). Taught within the School of Information Studies at Syracuse University, the course teaches programming fundamentals from the informatics perspective and is intended for non-computer science majors. There were 173 students enrolled in the course. The study only focused on the first 6 weeks because these units cover basic computational literacy constructs as applied to the Python programming language. These include instruction sequencing, variables, branching, iteration, and composition (functions). The course schedule can be found here: (<a href="https://ist256.com/spring2025/syllabus/#course-schedule"><span class="underline">https://ist256.com/spring2025/syllabus/#course-schedule</span></a>). The study was exempt from IRB under section §46.104 section 1, which covers research conducted in an established educational setting. A university IRB Exemption application has been filed and obtained for category 1 for research in established or commonly accepted educational settings. The IRB# is 24-346. While all elements of the study design were part of the course, students could elect to opt-out of including their data in the study.</p>
<p>To better understand the complexities of AI use in learning, a variety of research methods were adopted. Measuring the impact of AI use on exam scores used a correlational design CITE. Computational literacy was measured using a pretest-posttest design to establish a baseline prior to the intervention. To measure the impacts of in-context learning a between-subjects design was employed with randomly-assigned control and treatment groups. Analysis of chat activity among participants was crucial for</p>
<p>The study begins with a diagnostic instrument C1 to get the baseline of computational literacy of each participant. This happened within the first week of class before any instruction. At this time students were introduced to the course-provided AI, (<a href="https://ai.ist256.com"><span class="underline">https://ai.ist256.com</span></a>), which was a Large Language Model (GPT4o-mini) configured with a system prompt. Students were encouraged to use the AI as a virtual tutor asking it for help with Python questions and course-related assignments. When asking a specific question about an assignment, students were instructed to switch the LLM context by selecting the assignment in question from a drop-down menu.<br />
<img src="./publish/thesis2/html/media/image12.png" style="width:5.09375in;height:5.61458in" /><br />
<em>Figure 2: Context-Selection from the IST256 AI Tutor</em></p>
<p>For the control group T1 this action did nothing - it does not add any additional context. For the treatment group T2 the action copied the assignment or lab instructions into the conversational context.<br />
<img src="./publish/thesis2/html/media/image18.png" style="width:6.5in;height:4in" /><br />
<em>Figure 3: The treatment group (T2) is aware of the selected content.</em></p>
<p>Because the chatbot was self-hosted, all student interactions and AI responses were captured. D1 chatbot trace data is a dataset of those collected interactions throughout the six weeks of use. After the six week period, there were three observations. E1 was the midterm exam in the course covering the content from the first six weeks, C2 was a re-issue of the same C1 diagnostic as a means to measure improvement of computational literacy. Observation Q1 was a questionnaire that asked for simple demographic data about each participant. Q1 was issued at the end of the course along with course evaluations.</p>
<h3 id="participant-funnel">3.2.2 Participant Funnel</h3>
<p>Participants were students recruited from a Spring 2025 section of IST256. There were 173 students enrolled in the course. The research was exempt from IRB under section §46.104 section 1, which covers research conducted in an established educational setting. A university IRB Exemption application was filed and approved for category 1 for research in established or commonly accepted educational settings under the Syracuse University IRB number #24-346.</p>
<p>Of the 173 participants only 126 consented to the study. Five of the students who consented did not complete the observations necessary for the dependent variables: C1, C2 or E1, leaving 121 participants.</p>
<h4 id="population-1-chatbot-participants">3.2.2.1 Population 1: Chatbot participants </h4>
<p>There are two populations under analysis. The first population is the set of participants who used the AI chatbot and therefore have the trace data D1 necessary to study their AI interactions. There were 48 individuals in the control group and 39 in the treatment group for a total participant population of 87.</p>
<p><img src="./publish/thesis2/html/media/image3.png" style="width:6.5in;height:3.58333in" /><em>Figure 4: The participant funnel for chatbot use. 87 participants.</em></p>
<h4 id="population-2-chatbot-participants-with-survey-responses">3.2.2.1 Population 2: Chatbot participants with Survey Responses</h4>
<p>A survey Q1 was issued to students with the goal of identifying possible covariates. Besides demographic questions around the year of study, major, and gender students were also asked about their programming experience prior to the IST256 course. Ten chatbot users did not complete this survey thus reducing the participant size down to 77 whenever survey responses were needed in the analysis. Among the 77 participants, 41 were in the control group and 36 were in the treatment group.</p>
<p><img src="./publish/thesis2/html/media/image16.png" style="width:6.5in;height:3.05556in" /><em>Figure 5: The participant funnel when accounting for survey responses.</em></p>
<h3 id="computational-literacy-instrument-c1c2">3.2.3 Computational Literacy Instrument (C1/C2)</h3>
<p>No consensus models exist for developing computational thinking <a href="https://www.zotero.org/google-docs/?DLRbui">(Shute et al., 2017)</a>, therefore no commonly accepted instrument for measuring computational thinking exists. <a href="https://www.zotero.org/google-docs/?EpJMU8">Brennan &amp; Resnick, (2012)</a> assert three key dimensions of the computational thinking framework: computational concepts, computational practices, and computational perspectives. Computational concepts are programming structures like loops and conditions, debugging and remixing are examples of computational practices, and activities such as expressing and framing problems are examples of computational perspectives. The psychometrically validated CT-Test instrument developed by <a href="https://www.zotero.org/google-docs/?Tkr7qs">Román-González et al., (2017)</a> measures computational concepts only. The computational concepts evaluated through the instrument such as loops, branching, arrays, functions, instruction sequencing aligned closely with the computational concepts taught in the first 6 weeks of content in the IST256 course.</p>
<p>The CT-Test consists of twenty-eight multiple-choice questions, and students are given 45 minutes to complete the test. The test questions were delivered online one question at a time in a random order using Syracuse University’s Blackboard learning management system. Like the author’s original instrument, students were permitted to skip questions and revisit them later. The second test C2 had the same questions as C1 only the order of the questions was re-randomized for each student. Students received a score out of 28 on the test but no indication of what questions they got correct / incorrect.</p>
<p>The following figure is an example question from the CT-Test. The entire instrument can be found in appendix A.</p>
<p><img src="./publish/thesis2/html/media/image5.png" style="width:6.5in;height:3.16667in" /><br />
<em>Figure 6: A sample question fro the CT-Test</em></p>
<h3 id="chatbot-design">3.2.4 Chatbot Design</h3>
<p>The AI chatbot website used in the experiment (<a href="https://ai.ist256.com"><span class="underline">https://ai.ist256.com</span></a>) was programmed in Python using the Streamlit framework (<a href="https://streamlit.io/"><span class="underline">https://streamlit.io/</span></a>). It was released as open source under the Apache 2.0 license and published to Github (<a href="https://github.com/cent-ischool/ist256-chatapp"><span class="underline">https://github.com/cent-ischool/ist256-chatapp</span></a>). The application was deployed to a Kubernetes cluster in the Syracuse University data center. Users were required to authenticate with their Syracuse University credentials to use the application. Their chat interaction trace data was stored in a Postgresql database in the same data center. A python script was written to extract the data from the database.</p>
<h4 id="llm-selection">3.2.4.1 LLM Selection</h4>
<p>Acceptance criteria for choosing an LLM for the AI chatbot came down to accuracy and response time. I wanted the accuracy and response time to be on-par with the hosted platforms at that time: ChatGPT ((<a href="https://chatgpt.com/"><span class="underline">https://chatgpt.com/</span></a>) and Claude (<a href="https://claude.ai/"><span class="underline">https://claude.ai/</span></a>), so that participants would not churn based on those criteria.</p>
<p>Open-source models such Dolphin3, Llama3 and Qwen2.5 were considered initially. Unfortunately their response times fell off significantly at scale, likely due to these models being hosted on time-shared Syracuse-University GPU hardware. Their accuracy was found to be suitable for the content of the course, however.</p>
<p>Ultimately, the large language model selected Open AI’s GPT4o-mini. The model, which strikes a balance between price, response time and performance, had an adequate understanding of introductory Python, and since it was hosted in the Azure cloud it handled concurrent text generations. GPT4o was also evaluated, but since it performed similarly, at 16 times the price of 4o-mini, the latter seemed an obvious choice.</p>
<p>GPT4o-mini was a very adequate model. With correct prompting, it was able to generate correct solutions to all programming assignments and labs in the course. It also generated correct responses to 43 of 45 questions on the midterm exam (E1), placing it in the 98th percentile.</p>
<h4 id="random-assignment">3.2.4.2 Random Assignment</h4>
<p>Once authenticated to the AI chatbot website, participants were assigned to the control or treatment groups based on the MD5 hash of their student ID being odd or even. This method assured each student was always assigned to the same group at each login session. The algorithm is provided here:</p>
<p>hash = md5(student_id)<br />
number = int(hash)<br />
in_treatment = mod(number, 2) # odd or even</p>
<h4 id="control-group-t1">3.2.4.3 Control Group (T1)</h4>
<p>Every request to the AI Tutor included a system prompt. The system prompt was used to place some guardrails on the AI and control the generated output. The system prompt’s objectives were set to: (1) generate Python code in the style which is taught in the course, (2) explain all code generated written, (3) adopt a persona of a helpful and friendly tutor for a college level introductory Python course, and (4) not answer questions that are not course related.</p>
<p>Several system prompt iterations were evaluated before the following was determined to be most suitable for meeting the objectives while minimizing the number of input tokens.This was the base model configuration for the control group (T1).<br />
<img src="./publish/thesis2/html/media/image7.png" style="width:6.5in;height:2in" /><br />
<em>Figure 7: The system prompt for the control group T1.</em></p>
<h4 id="treatment-group-t2">3.2.4.4 Treatment Group (T2)</h4>
<p>The treatment group consisted of the control group LLM and configurations (T1) plus a user prompt injected into the conversation based on contextual selection. For example, if the participant selected <strong>03-HW-Conditionals</strong> as the context (assignment), the content of the homework assignment notebook <strong>lessons/03-Conditionals/HW-Conditionals.ipynb</strong>, was loaded into the chat conversation. This was the same assignment to be completed by the student and contains the instructions, suggested approach, sections where code must be written and any sample code. The following prompt template is used to add the information to the conversation:</p>
<p><img src="./publish/thesis2/html/media/image23.jpg" style="width:6.5in;height:1.34722in" alt="A black screen with white text AI-generated content may be incorrect." /></p>
<p><em>Figure 8: Treatment T2 context prompt template.</em></p>
<p>Subsequently, the AI chatbot responds that it is ready to assist with the assignment. The AI response demonstrated context awareness.</p>
<p><img src="./publish/thesis2/html/media/image13.jpg" style="width:6.5in;height:1.36111in" alt="A black background with white text AI-generated content may be incorrect." /></p>
<p><em>Figure 9: AI response to context selection from T2.</em></p>
<p>When a participant asked the AI to complete a specific section of the assignment, the AI responded with working code and with an explanation of it. This is a key difference between T1 and T1. The control group T1 could generate the working code as well, but the participant must elicit the prompt to generate the results. This could be achieved by copying the assignment instructions into the prompt, which was all T2 was doing essentially.</p>
<p>The purpose of the control / treatment group was to help understand the impact of context-awareness on learning. For example, did the context-awareness improve user engagement with AI, or simply make question-answering more convenient?</p>
<h3 id="midterm-exam-e1">3.2.5 Midterm Exam (E1)</h3>
<p>The purpose of the midterm exam was to measure students’ understanding of the fundamental concepts in the course. The exam covered:</p>
<p>1. Conceptual ideas, e.g. Definite and indefinite loops</p>
<p>2. Application of the concepts, e.g. when does one use a definite vs indefinite loop?</p>
<p>3. Concept in Python language, e.g. how does one use while or for to write loops? Is the loop you see a definite or indefinite loop?</p>
<p>4. Application in Python language, e.g. here is code with a while loop. What is the output upon execution?</p>
<p>The exam contained 45 multiple choice questions and students were given 45 minutes to complete it. The exam was closed book with the exception of a single page of notes as a study guide. Exam and academic integrity were preserved by issuing the exam in class and on paper. There were 54 versions of the same exam, with each version having both questions and answers randomized. Versions A-D were issued in class, version E was reserved for students at the testing center.</p>
<p>The Cronbach’s Alpha <a href="https://www.zotero.org/google-docs/?WLDVx1">(Cronbach, 1951)</a> of the exam versions was between 0.83 and 0.88, indicating high reliability the instrument reflects students’ subject knowledge.</p>
<table>
<thead>
<tr class="header">
<th>Exam Version (E1)</th>
<th>N</th>
<th>Cronbach’s Alpha</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>40</td>
<td>0.87</td>
</tr>
<tr class="even">
<td>B</td>
<td>40</td>
<td>0.86</td>
</tr>
<tr class="odd">
<td>C</td>
<td>40</td>
<td>0.83</td>
</tr>
<tr class="even">
<td>D</td>
<td>39</td>
<td>0.87</td>
</tr>
<tr class="odd">
<td>E</td>
<td>9</td>
<td>0.88</td>
</tr>
<tr class="even">
<td></td>
<td>168</td>
<td></td>
</tr>
</tbody>
</table>
<p><em>Table 1: Cronbach’s Alpha of E1 midterm exam</em></p>
<h3 id="questionnaire-q1">3.2.6 Questionnaire (Q1)</h3>
<p>A debriefing questionnaire (Q1) was issued after the experiment. The purpose of this questionnaire was to:</p>
<p>1. Collect basic demographics from the participant pool. Such as gender, major, and class rank.</p>
<p>2. Identify participants with prior computer programming experience.</p>
<p>3. Identify participants who used AI other than the AI provided.</p>
<p>4. Collect student perceptions of programming and AI use.</p>
<p>The questionnaire was delivered using the Syracuse University Qualtrics survey system and can be found in Appendix B. Only demographic data and responses to prior programming experience were used in the data analysis.</p>
<h3 id="chatbot-trace-data-d1">3.2.7 Chatbot Trace Data (D1)</h3>
<p>As explained earlier the AI chatbot recorded participant interactions and AI responses into a Postgresql database. The data included when they interacted with the AI, the context they selected, if they were in the control or treatment group. Here’s a data dictionary of the fields in the trace data:</p>
<table>
<thead>
<tr class="header">
<th><strong>Field</strong></th>
<th><strong>Data Type</strong></th>
<th><strong>Purpose</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Id</td>
<td>Sequential integer</td>
<td>A unique number for each item in the trace data, the higher the number the more recent the interaction.</td>
</tr>
<tr class="even">
<td>Session Id</td>
<td>Universally Unique ID (UUID)</td>
<td>A globally unique identifier to record the chat session. Filtering on a specific session Id will produce a conversation between user and the AI assistant.</td>
</tr>
<tr class="odd">
<td>Participant ID</td>
<td>Text</td>
<td>The participant number. Unique for each participant, so that AI use can be traced back to observations C1, C2, E1 and questionnaire Q1.</td>
</tr>
<tr class="even">
<td>Timestamp</td>
<td>ISO8601 timestamp</td>
<td>The UTC timestamp user’s prompt or AI response as text string in IST8601 format</td>
</tr>
<tr class="odd">
<td>Model</td>
<td>Text</td>
<td>The AI Model used. This is always gpt-4o-mini</td>
</tr>
<tr class="even">
<td>Rag</td>
<td>Boolean</td>
<td>True is the treatment group T1 whereas False is the control group T2</td>
</tr>
<tr class="odd">
<td>Context</td>
<td>Text</td>
<td>The user selected context at the time of the user prompt or AI response</td>
</tr>
<tr class="even">
<td>Role</td>
<td>Text</td>
<td>Values “user” or “assistant” Indicator of whether the row was a user prompt or an AI response.</td>
</tr>
<tr class="odd">
<td>Content</td>
<td>Text</td>
<td>In the case of Role == “user”, the Content is the prompt. When the Role == “assistant”, the content is the AI generated response.</td>
</tr>
</tbody>
</table>
<p><em>Table 2: Fields in the D1 Chatbot Trace Data</em><br />
The trace data could be coded in a variety of ways, such as based on the type of question asked, or the specifically of the prompt. Ultimately this trace data was used to classify users' interactions with the AI by session Id, then quantify those session-based interactions.</p>
<p>There were 9,518 entries in the D1 chatbot trace data. The granularity of the trace data was one row per user prompt or AI response. To analyze the data based on session - a series of requests and responses representative of a conversation - the data was grouped by session id, and sorted by timestamp. Each session was then assembled into a conversation thread consisting of the user prompt and AI responses. This resulted in 1024 sessions.</p>
<h2 id="data-analysis">3.3 Data Analysis</h2>
<h3 id="overview-1">3.3.1 Overview </h3>
<p>The following section outlines the methods used to operationalize the D1 chatbot trace data. Because all three research questions necessitated use of the trace data for answering the research questions, this data played a pivotal role in the study. Categorical content analysis <a href="https://www.zotero.org/google-docs/?imAA0t">(Neuendorf, 2017)</a> was used to classify student AI interactions into two categories of learning-supportive and learning-avoidance activities. The classification unit boundary was defined as each participants' chat session from login to logout. The trace data would be coded at the session level (n=1024) and the codes would be counted by participant for analysis at the participant level (n=87).</p>
<p>Before categorical content analysis could be performed, a codebook had to be built. I built the codebook deductively based on observations of other researchers in literature in addition to my seven years of experience teaching the course. I then coded a random sample of 50 sessions from the 1024 sessions in total using the codebook I constructed. Next I converted the codebook into an AI prompt so I could use an LLM to code the other sessions similar to work done by <a href="https://www.zotero.org/google-docs/?KqI6Sg">Xiao et al., (2023)</a>. <a href="https://www.zotero.org/google-docs/?0N4LQE">Pilny et al., (2024)</a> discovered that not all LLMs perform equally well at content analysis, and for this reason four different models were evaluated against my human-coded sample of 50 sessions. Coding consistency is also important and some LLMs have shown their outputs to be variable over time <a href="https://www.zotero.org/google-docs/?166nDy">(Hackl et al., 2023)</a>. To mitigate this issue each candidate LLM would execute the same content analysis of the 50 sample sessions three times. Krippendorff’s alpha reliability measure <a href="https://www.zotero.org/google-docs/?iDryVo">(Krippendorff, 2011)</a> was used to validate the inter-coder reliability among the LLM when compared to my coding and with respect to itself.</p>
<p>Once the best model was determined, all 1024 sessions were classified and then counted in buckets of “Learning” or “Task Completion” at the participant to match the unit of analysis for addressing the research questions.</p>
<h3 id="tools">3.3.2 Tools</h3>
<p>Data analysis was performed using the Python programming language, using Jupyter notebooks. Data visualizations were created with the help of the matplotlib, seaborn, sankeyflow and forestplot libraries. Statistical analysis was accomplished with the help of the pandas, scipy, statsmodels, pingouin, and simpledorff libraries. [QUESTION: Do I cite these]</p>
<p>Openrouter (<a href="https://openrouter.ai"><span class="underline">https://openrouter.ai</span></a>) was used to provide unified API level access to different Large-Language Models across platforms like OpenAI, xAI, Google and Anthropic.</p>
<p>To protect participants, the source data and any processed data sets were stored with the code in a private Git repository. This repository was backed up to a personal storage device. Final code and data for this research will be accessible on a private Github repository at <a href="https://github.com/mafudge/dps-thesis"><span class="underline">https://github.com/mafudge/dps-thesis</span></a> until it can be determined the data may be shared publicly. The final data will be scrubbed of all personally-identifiable information which can link the data back to an individual.</p>
<h3 id="operationalizing-d1-chatbot-trace-data">3.3.3 Operationalizing D1 Chatbot Trace Data </h3>
<p>To perform the categorical content analysis of the 1024 D1 chatbot trace data sessions, first a codebook was built deductively. An initial set of codes were established based on observations of other researchers in literature in combination with my seven years of experience teaching IST256. A sub-sample of 50 sessions were selected at random from the D1 dataset to identify initial classifications. I classified each session according to the codebook in Table 3. In addition, I summarized the classified activity as an example which could be incorporated into the AI prompt used by the LLM.</p>
<table>
<thead>
<tr class="header">
<th><strong>Participant<br />
Behavior</strong></th>
<th><strong>Definition</strong></th>
<th><strong>Example</strong></th>
<th><strong>Classified<br />
Activity</strong></th>
<th><strong>Supported<br />
Citations</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Seeking Answers (Superficial Learning)</td>
<td>The participant makes a direct ask of the AI to complete their assignment or lab. There is no conversation.</td>
<td><p>"Can you code the solution for assignment ABC?"</p>
<p>"How do I complete section XYZ in the lab?"</p>
<p><a href="https://www.zotero.org/google-docs/?ocgJMx">(Hassan et al., 2025)</a></p></td>
<td>Task Completion</td>
<td><p><a href="https://www.zotero.org/google-docs/?EeccHE">(Finnie-Ansley et al., 2022)</a></p>
<p><a href="https://www.zotero.org/google-docs/?M8nObm">(Hassan et al., 2025)</a></p></td>
</tr>
<tr class="even">
<td><p>Overreliance</p>
<p>(Cognitive offloading for task completion)</p></td>
<td>The participant asks the AI to complete a task and does not further engage. Thus there is no clear intention to understand their work for them. Thus a deep connection to the material is not formed.</td>
<td><p>"Can you convert this algorithm into code?"</p>
<p>"Can you provide an approach to start this assignment?"</p>
<p>"Fix this error for me."</p>
<p><a href="https://www.zotero.org/google-docs/?fWvkuI">(Prather, Reeves, Leinonen, et al., 2024)</a></p></td>
<td>Task Completion</td>
<td><p><a href="https://www.zotero.org/google-docs/?b94RCp">(Prather, Reeves, Leinonen, et al., 2024)</a></p>
<p><a href="https://www.zotero.org/google-docs/?afJ6YN">(Margulieux et al., 2024)</a></p></td>
</tr>
<tr class="odd">
<td><p>Scaffolding</p>
<p>(AI as Expert)</p></td>
<td>AI provides support / expertise to the learner to a learner to help them achieve a task they couldn't complete on their own.</td>
<td><p>"What does this error mean?"</p>
<p>"Can you help me locate the error in my code?"</p>
<p>"Can you help me troubleshoot my code?"</p>
<p>.<a href="https://www.zotero.org/google-docs/?CjRe5C">(Prather, Reeves, Denny, et al., 2024)</a></p></td>
<td>Learning</td>
<td><p><a href="https://www.zotero.org/google-docs/?wDe1Ck">(Prather et al., 2019)</a></p>
<p><a href="https://www.zotero.org/google-docs/?svZdyf">(Prather, Reeves, Denny, et al., 2024)</a></p>
<p><a href="https://www.zotero.org/google-docs/?vorDax">(Hassan et al., 2025)</a></p></td>
</tr>
<tr class="even">
<td><p>Questioning</p>
<p>(AI as Tutor)</p></td>
<td>Asking the AI to explain a concept or provide code examples of a concept.</td>
<td><p>Can you provide an example of a nested loop?</p>
<p>Why would I use a nested IF verses elif?</p>
<p>Can you help me understand dictionaries?</p>
<p>When would I use a list versus a dictionary?</p></td>
<td>Learning</td>
<td><p><a href="https://www.zotero.org/google-docs/?nTmux6">(Finnie-Ansley et al., 2022)</a></p>
<p><a href="https://www.zotero.org/google-docs/?NzDiBi">(Cambaz &amp; Zhang, 2024)</a></p></td>
</tr>
<tr class="odd">
<td>Unable to Determine</td>
<td>The transcript does not provide enough information to classify the participant's behavior. Off topic question not about the subject matter.</td>
<td>"When is the exam?"&lt;br&gt;"Can you tell me the weather?"&lt;br&gt;What did I ask you last week?</td>
<td>Inconclusive</td>
<td></td>
</tr>
</tbody>
</table>
<p><em>Table 3: Codebook of participant behaviors</em></p>
<p>For example, in one of <em>Participant 119</em>’s sessions this was this only question asked the AI chatbot:</p>
<blockquote>
<p>PARTICIPANT_119: “Write a function, score_sentiment() which given 3 inputs: positive words, negative words and some text, will return an integer score of sentiment as output. Assume all inputs are lower case and do not have any punctuations.”<br />
AI_ASSISTANT: (writes code and explains what it does as the bot was designed)</p>
</blockquote>
<p>The participant pasted in part of the assignment instructions asking the AI generate code. I classified this session as Task Completion, under the behavior Seeking Answers.</p>
<p>For another example, here is one of <em>Participant 107’s</em> sessions. I classified this interaction as Learning under the behavior Questioning, as it is obvious the participant is seeking to understand Python concepts.</p>
<blockquote>
<p>PARTICIPANT_107: Why false?<br />
a=0.1<br />
b=0.2<br />
bool(a+b==0.3)<br />
AI_ASSISTANT: (provides an explanation with examples)<br />
<br />
PARTICIPANT_107: explain 7%2<br />
AI_ASSISTANT: (provides an explanation with examples)<br />
<br />
PARTICIPANT_107: explain == vs =<br />
AI_ASSISTANT: (provides an explanation with examples)</p>
</blockquote>
<p>An AI prompt was formulated from the codebook and the various examples of interactions I observed within the observations. The AI prompt is included in Appendix C.</p>
<h3 id="model-selection-reliability-testing">3.3.4 Model Selection Reliability Testing</h3>
<p>Using a Large-Language Model for content analysis raised two concerns. First was inter-coder reliability. I wanted the LLM classifier to match my 50 classifications as closely as possible. Secondly was internal reliability, LLMs are prone to hallucinations so I had to ensure whatever model I selected was as consistent as possible across multiple classifications of the same data. The best model would then be the one which was the most similar to my classification and the most internally consistent.</p>
<p>Four different models were chosen as candidates: x-ai/grok-4-fast, openai/gpt-5-mini, google/gemini-2.5-flash, and anthropic/claude-sonnet-4. Each model candidate came from a different AI provider and were considered appropriate for text-processing type tasks at the time of the study.</p>
<p>The same AI prompt from Appendix C was used across all four models, however the prompt in appendix C was not the initial prompt. It was refined to ensure accurate output representations across the models. To accomplish this task, my interpretation of the session was compared against the outputs of the 4 models, and the prompt was refined until there was consensus. This was repeated for each type of classification: Task Completion / Seeking Answer, Task Completion / Overreliance, Learning / Scaffolding, and Learning / Questioning. Four examples in total.</p>
<p>With an established AI prompt, I could now check for inter-coder reliability. Each of the four models was used to classify the same 50 sessions in the sub-sample I classified initially. My classifications were compared against the LLM’s classifications using the Krippendorff’s Alpha to obtain a measure of agreement for inter-coder reliability.</p>
<p>The inter-coder reliability procedure was repeated three times to establish a measure of consistency among model runs. Once again the Krippendorff’s alpha measure was used to measure the model’s agreement, but this time with itself across three runs. The expectation was an internal consistency of 1.0 which means the LLM performed the task identically each time.</p>
<table>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Agreement with Human</strong></th>
<th><strong>Internal Consistency<br />
(3 runs)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>x-ai/grok-4-fast</td>
<td>0.873131</td>
<td>0.885031</td>
</tr>
<tr class="even">
<td>openai/gpt-5-mini</td>
<td>0.788552</td>
<td>0.914743</td>
</tr>
<tr class="odd">
<td>google/gemini-2.5-flash</td>
<td>0.833052</td>
<td>1.000000</td>
</tr>
<tr class="even">
<td>anthropic/claude-sonnet-4</td>
<td>0.873131</td>
<td>1.000000</td>
</tr>
</tbody>
</table>
<p><em>Table 4: The Krippendorff’s Alpha of model vs human and internal consistency of model against itself.</em></p>
<p>As table 4 indicates, x-ai/grok-4-fast and anthropic/claude-sonnet-4 classifications were in closest agreement to my classifications, having the highest inter-coder reliability. Only google/gemini-2.5-flash, and anthropic/claude-sonnet-4 classified the data with perfect consistency across the three runs. For these reasons anthropic/claude-sonnet-4 was chosen as the model to complete the classification task performing highest in both categories.</p>
<p>Incidentally, the Krippendorff’s alpha across all models + human for the three runs was 0.76108, 0.75115 and 0.77207.</p>
<h3 id="categorical-content-analysis">3.3.5 Categorical Content Analysis</h3>
<p>With the identification of the best LLM for the task, I now used anthropic/claude-sonnet-4 to classify all 1024 sessions in the D1 chatbot trace dataset. In total there were 623 labeled “Learning”, 336 categorized as “Task Completion” and 35 categorized as “Inconclusive.” The final unit of analysis is based on participants, so counts of classifications were grouped across the 87 participants. For example, participant #82 had 33 sessions categorized as “Learning”, 7 sessions categorized as “Task Completion” and 2 categorized as “Inconclusive.” Participant #124 has 5 sessions categorized as “Learning”. 25 categorized as “Task Completion” and one categorized as “Inconclusive.” With the data setup this way I could now use the categorized counts as independent variables, and study their impacts on E1, C1 and C2.</p>
<p><img src="./publish/thesis2/html/media/image22.png" style="width:5.23438in;height:3.90012in" /></p>
<p><em>Figure 10: Three classifications of chat sessions from the D1 dataset</em></p>
<h2 id="hypothesis-formulation-and-methodology">3.4 Hypothesis Formulation and Methodology</h2>
<p>With the dataset established the analysis could commence. This section breaks down the original research questions into testable hypotheses and then explains the methodology used to answer the research questions.</p>
<h3 id="rq1-hypothesis-and-methodology">3.4.1 RQ1 Hypothesis and Methodology</h3>
<p>Research Question 1 states:</p>
<p><em>RQ1: How does large-language model use influence student learning performance?</em></p>
<p>This question investigated the relationship between the quantity of participant sessions classified as “learning” and “task completion” from the D1 dataset and their midterm exam scores E1.</p>
<p><strong>Alternate Hypothesis for H1</strong></p>
<p>The quantity and classification of participant sessions in the D1 dataset are significantly correlated with their midterm exam scores (E1). Specifically:</p>
<ol type="1">
<li><blockquote>
<p>There is a positive correlation between the quantity of sessions classified as “learning” and midterm exam scores.</p>
</blockquote></li>
<li><blockquote>
<p>There is a negative correlation between the quantity of sessions classified as "task completion" and midterm exam scores.</p>
</blockquote></li>
</ol>
<p><strong>Null Hypothesis for H1</strong></p>
<p>There is no statistically significant correlation between the quantity of participant sessions from the D1 dataset, whether classified as "learning" or "task completion," and their midterm exam scores (E1).</p>
<p><strong>Methodology</strong></p>
<p>A hierarchical multiple regression analysis <a href="https://www.zotero.org/google-docs/?DRcM77">(Raudenbush &amp; Bryk, 2002)</a> was employed to test the study hypotheses. This helped explain the contribution of each of the independent variables in addition to their collective impact on exam scores. The beta coefficients from the regression indicated the nature of the relationship such as positive or negative and the degree of influence of the independent variables. This strategy allows examination of each predictor's independent contribution before assessing their combined effects. This would help to reveal whether both variables provide unique information about the outcome. There are three models:</p>
<p><strong>Model 1 (Task completion only):</strong> E1 was regressed on task completion session count alone to establish the bivariate relationship.</p>
<p><strong>Model 2 (Learning only):</strong> E1 was regressed on learning session count alone to establish its bivariate relationship.</p>
<p><strong>Model 3 (Full model):</strong> E1 was regressed simultaneously on both task completion session count and learning session count to examine their independent effects when controlling for one another.</p>
<p>For each model, I reported regression coefficients (beta), standard errors, <em>t</em>-statistics, <em>p</em>-values, 95% confidence intervals, and overall model fit statistics (r-squared, adjusted r-squared, and F-statistic). Changes in r-squared across models were calculated to assess whether both predictors contributed unique variance beyond what either explained alone.</p>
<p>Methodological analysis with respect to H1:</p>
<ol type="1">
<li><blockquote>
<p>The "learning" beta coefficient being positive with a p-value &lt; 0.05, would indicate statistical significance. This implies there would be a positive correlation between the quantity of sessions classified as “learning” and midterm exam scores (E1).</p>
</blockquote></li>
<li><blockquote>
<p>The “task completion” beta coefficient being negative with a p-value &lt; 0.05 would imply a negative correlation between the quantity of sessions classified as "task completion" and midterm exam scores (E1).</p>
</blockquote></li>
</ol>
<p>Both conditions 1 and 2 must be satisfied to reject the null hypothesis for H1.</p>
<h3 id="rq2-hypothesis-and-methodology">3.4.2 RQ2 Hypothesis and Methodology</h3>
<p>Research Question 2 states:</p>
<p>RQ2: When the prompt is adjusted to include assignment instructions (in-context learning) what is the impact on student learning performance?</p>
<p>This question investigated the impact of in-context learning on midterm exam scores E1. The assignment instructions were added to LLM’s context of participants in the treatment group. This meant the LLM was aware of assignment-related questions.</p>
<p><strong>Alternate Hypothesis for H2</strong></p>
<p>The mean student learning performance score (E1) for the treatment group, which used the context-aware AI, will be statistically significantly higher than the mean score for the control group.</p>
<p><strong>Null Hypothesis for H2</strong></p>
<p>There is no statistically significant difference in the mean student learning performance scores (E1) between the treatment group and the control group.</p>
<p><strong>Methodology</strong></p>
<p>A between-subjects experimental design <a href="https://www.zotero.org/google-docs/?rvvwq7">(Creswell &amp; Creswell, 2017: ch8)</a> was employed with participants assigned at random to either a control group (T1) or treatment group (T2), which employed using context-aware AI. The dependent variable was midterm exam score (E1), measured on a continuous scale. The independent variable—group assignment—was dummy coded (0 = control, 1 = treatment) for regression analysis.</p>
<p>Linear regression was selected as the primary analytical approach, with the beta coefficient representing the mean difference in exam scores between control and treatment. Statistical significance was assessed using p-values &lt;= .05.</p>
<p>A multiple linear regression with learning session count and task completion session counts added as covariates will help to identify any suppression effects. Mediation and moderation analyses <a href="https://www.zotero.org/google-docs/?xe6s9J">(Edwards &amp; Lambert, 2007)</a> were also conducted to understand the underlying mechanisms that session counts of task completion and learning might have on the treatment.</p>
<p>A multiple linear regression with learning session count and task completion session counts was used to check for suppression effects:</p>
<p><strong>Model 1 (Learning Session Count + Task Completion Count)</strong> Does the addition of AI use change the results of the original model?</p>
<p>Mediation analysis was used to explore whether the treatment effect operated through changes in student usage behaviors:</p>
<p><strong>Model 2a (Treatment → Learning Session Count)</strong> Does control/treatment group assignment predict learning session count?</p>
<p><strong>Model 2b (Treatment → Task Completion Session Count)</strong> Does control/treatment group assignment predict task completion session count?</p>
<p>Moderation analysis examined whether the treatment condition altered the strength of relationships between usage patterns and exam performance.</p>
<p><strong>Model 3a (Treatment x Learning Session Count)</strong> This model tested whether context-aware AI moderated the relationship between learning session engagement and exam performance.</p>
<p><strong>Model 3b (Treatment x Task Completion Session Count)</strong> This model tested whether context-aware AI moderated the relationship between task completion session engagement and exam performance.</p>
<h3 id="rq3-hypothesis-and-methodology">3.4.3 RQ3 Hypothesis and Methodology</h3>
<p>Research Question 3 states:</p>
<p><em>RQ3: What is the relationship between large language model use and computational literacy?</em></p>
<p>This question investigated the relationship between the quantity of participant sessions classified as “learning” and “task completion” from the D1 dataset and their computational literacy scores (C1, C2).</p>
<p><strong>Alternate Hypothesis for H3</strong></p>
<p>The quantity and classification of participant sessions in the D1 dataset are significantly correlated with their computational literacy scores (C1, C2). Specifically:</p>
<ol type="1">
<li><blockquote>
<p>There is a positive correlation between the quantity of sessions classified as “learning” and computational literacy scores.</p>
</blockquote></li>
<li><blockquote>
<p>There is a negative correlation between the quantity of sessions classified as "task completion" and computational literacy scores.</p>
</blockquote></li>
</ol>
<p><strong>Null Hypothesis for H3</strong></p>
<p>There is no statistically significant correlation between the quantity of participant sessions from the D1 dataset, whether classified as "learning" or "task completion," and their computational literacy scores (C1, C2).</p>
<p><strong>Methodology</strong></p>
<p>To measure test-retest reliability a Pearson correlation will be measured against C1 and C2 <a href="https://www.zotero.org/google-docs/?sedsAX">(Vaz et al., 2013)</a>. To account for baseline computational literacy and test-retest effects from repeated administration of the same assessment from C1 to C2, I employed an analysis of covariance (ANCOVA). Specifically, I used linear regression with midterm computational literacy scores (C2) as the dependent variable with the baseline scores (C1), task completion session count, and learning session count as the independent variables <a href="https://www.zotero.org/google-docs/?lwb3sB">(Huitema, 2011)</a>. This approach controls for individual differences in prior knowledge as measured on C1 while examining whether usage patterns predict final outcomes beyond what would be expected from baseline ability alone (<a href="https://www.zotero.org/google-docs/?2QVeNI">Maxwell et al., 2017</a>).</p>
<p>I also conducted a supplementary analysis using the change in diagnostic scores (C2 - C1) as the dependent variable to examine whether LLM usage patterns predicted the magnitude of improvement. This was a complementary approach as change scores low reliability when pre- and post-test correlations are high <a href="https://www.zotero.org/google-docs/?m6FKwM">(Cronbach &amp; Furby, 1970)</a>. Both analyses together offered a comprehensive understanding of the relationship between LLM usage and computational literacy development.</p>
<p>Methodological analysis with respect to H1:</p>
<ol type="1">
<li><blockquote>
<p>The "learning" beta coefficient being positive with a p-value &lt; 0.05, would indicate statistical significance. This implies there would be a positive correlation between the quantity of sessions classified as “learning” and computational literacy scores (C2).</p>
</blockquote></li>
<li><blockquote>
<p>The “task completion” beta coefficient being negative with a p-value &lt; 0.05 would imply a negative correlation between the quantity of sessions classified as "task completion" and computational literacy scores (C2).</p>
</blockquote></li>
</ol>
<p>Both conditions 1 and 2 must be satisfied to reject the null hypothesis for H3.</p>
<h3 id="satisfying-assumptions-of-linear-regressions">3.4.4 Satisfying Assumptions of Linear Regressions</h3>
<p>For every linear regression, tests for linear assumptions were applied to ensure the reliability of the results <a href="https://www.zotero.org/google-docs/?Z880H3">(Cohen et al., 2013; Osborne &amp; Waters, 2002)</a> . Residual plots and regression plots were observed for linearity. To determine if the variance of the residuals were constant across all levels of the independent variables, the Breusch-Pagan test for homoscedasticity was used <a href="https://www.zotero.org/google-docs/?8K0xlP">(Breusch &amp; Pagan, 1979)</a>. The Durbin-Watson statistic was calculated to check for autocorrelation <a href="https://www.zotero.org/google-docs/?uQj75M">(Durbin &amp; Watson, 1971)</a>. To verify the residuals were normally distributed, Shapiro-Wilk test was performed <a href="https://www.zotero.org/google-docs/?XBdLM8">(Shaphiro &amp; Wilk, 1965)</a>. And finally, to check for conditions of multicollinearity <a href="https://www.zotero.org/google-docs/?4mopcJ">(Farrar &amp; Glauber, 1967)</a> among independent variables, the variance inflation factor metric was calculated .</p>
<p>Diagnostic testing that revealed heteroscedasticity, or non-constant error variance across the range of fitted values, would be regressed with robust standard errors. This ensured valid statistical inference in the presence of heteroscedasticity <a href="https://www.zotero.org/google-docs/?m8VdRu">(MacKinnon &amp; White, 1985)</a>. This approach provides robust hypothesis tests without requiring the homoscedasticity assumption, yielding accurate <em>p</em>-values and confidence intervals even when error variance is non-constant.</p>
<h1 id="results"><strong>4.0 Results</strong></h1>
<h2 id="introduction-2">4.1 Introduction</h2>
<p>The findings of my research will be discussed in this section. [TODO: Write up a summary of findings for this section, once all the details have been written up.]</p>
<h2 id="findings-for-rq1">4.2 Findings for RQ1</h2>
<h3 id="model-overview-for-rq1">4.2.1 Model Overview for RQ1</h3>
<p>This research question investigated whether the quantity and classification of student sessions with large language models predicted midterm exam performance (E1). Specifically, I examined whether learning session count and task completion sessions count were differentially associated with academic outcomes (<em>N</em> = 87).</p>
<p>The regression model used to test the hypothesis had learning session count and task completion count as independent variables, with midterm exam E1 as the dependent variable.</p>
<p><strong>Model 3: E1 ~ Learning Session Count + Task Completion Session Count</strong></p>
<p><strong>Alternate Hypothesis for H1</strong></p>
<p>The quantity and classification of participant sessions in the D1 dataset are significantly correlated with their midterm exam scores (E1). Specifically:</p>
<ol type="1">
<li><blockquote>
<p>There is a positive correlation between the quantity of sessions classified as “learning” and midterm exam scores.</p>
</blockquote></li>
<li><blockquote>
<p>There is a negative correlation between the quantity of sessions classified as "task completion" and midterm exam scores.</p>
</blockquote></li>
</ol>
<p><strong>Null Hypothesis for H1</strong></p>
<p>There is no statistically significant correlation between the quantity of participant sessions from the D1 dataset, whether classified as "learning" or "task completion," and their midterm exam scores (E1).</p>
<p>Both conditions 1 and 2 must be satisfied to reject the null hypothesis for H1.</p>
<h3 id="regression-assumption-tests-of-the-model-for-rq1">4.2.2 Regression Assumption Tests of the Model for RQ1</h3>
<p>A comprehensive assessment of ordinary least squares (OLS) regression assumptions was conducted prior to interpreting the findings of the model for RQ1. These checks assured the validity and reliability of the model estimates.</p>
<p>To check for linearity, partial regression plots were examined to assess the linear relationship between the dependent variable (E1) and each predictor variable while controlling for other predictors in the model. Visual inspection of these plots indicated approximate linear relationships, supporting the use of linear regression for analysis. Both independent variables had fewer observations as the counts of the participants’ sessions increased. This was expected for session-oriented data which is commonly right-skewed, where most participants have few sessions and few participants have several sessions..<img src="./publish/thesis2/html/media/image8.png" style="width:6.5in;height:4.48611in" /><br />
<em>Figure ??: Partial regression plots when holding the other constant of E1 ~ Task Completion Session Count + Learning Session Count.</em></p>
<p>The plot of residuals versus fitted values showed symmetrical, randomly scattered points with no obvious pattern. The Locally Estimated Scatterplot Smoothing (LOESS) curve was relatively flat and close to the horizontal. The end of the LOESS curve does indicate a slight uptick likely due to lack of observations in that region.</p>
<p><img src="./publish/thesis2/html/media/image11.png" style="width:5.22917in;height:3.46875in" /><br />
<em>Figure ??: Residuals vs Fitted Values for E1 ~ Learning Session Count + Task Completion Session Count</em></p>
<p>The Durbin-Watson statistic (DW = 2.09) was within the acceptable range, falling close to the ideal value of 2.0. This indicated no substantial autocorrelation among residuals, thereby satisfying assumption of independence.</p>
<p>The Breusch-Pagan test for heteroscedasticity yielded a non-significant result (chi-square = 0.29, p-value = .867). I interpreted this as the assumption of constant error variance across levels of the predictor variables was met. This was further supported by visual inspection of the residuals versus fitted values plot, which showed no clearly discernible pattern or funnel shape.</p>
<p>The Shapiro-Wilk test indicated that residuals were normally distributed (W = 0.99, p-value = 0.679). Since p&gt;=0.05 we fail to reject the null hypothesis of normality. This was corroborated visually via a histogram of residuals and the Q-Q plot, both of which demonstrated close adherence to the normal distribution.</p>
<p><img src="./publish/thesis2/html/media/image15.png" style="width:6.5in;height:2.30556in" /><br />
<em>Figure ??: Evidence of normality among the residuals of E1 ~ Task Completion Session Count + Learning Session Count.</em></p>
<p>To check for multicollinearity, the variance inflation factor (VIF) test was performed. Values for both predictor variables were well below conventional thresholds of concern (VIF = 1.19 for both task completion session count and learning session count), indicating no problematic multicollinearity.</p>
<p>Given that all diagnostic tests indicated satisfaction of OLS regression assumptions, the model was deemed appropriate for interpretation. The model explained 18.9% of the variance in E1 (r-squared = 0.189, adjusted r-squared = .169), and was statistically significant overall (F(2, 84) = 9.76, p-value &lt; 0.001).</p>
<h3 id="hierarchical-regression-analysis-for-the-rq1-model">4.2.3 Hierarchical Regression Analysis for the RQ1 Model</h3>
<p>Prior to testing the full regression model, bivariate relationships between predictor variables and midterm exam scores (E1) were examined through simple linear regression analyses. This preliminary step allows for assessment of individual predictor effects before examining their combined contribution as a means to gain a better understanding of each independent variables’ contribution to the model.</p>
<h4 id="model-1-e1-task-complection-session-count">4.2.3.1 Model 1: E1 ~ Task Complection Session Count</h4>
<p>A simple linear regression revealed that task completion session count was a significant negative predictor of E1 (<em>F</em>(1, 85) = 12.35, <em>p</em> &lt; 0.001). The model explained 12.7% of variance in exam scores (r-squared = 0.127, adjusted r-squared = 0.117). The model met statistical power assumptions for a power of 0.8 and an alpha of 0.05, with Cohen's f-squared exceeding the target. (Actual Cohen’s f-squared = 0.145 &gt; Target Cohen’s f-squared = 0.092). Each additional task completion session resulted in an E1 decrease by 0.41 units (beta = -0.41, SE = 0.12, <em>t</em> = -3.52, <em>p</em> = .001, 95% CI [-0.65, -0.18]). This bivariate relationship confirmed greater engagement with task completion sessions was independently associated with lower exam scores.<br />
<img src="./publish/thesis2/html/media/image17.png" style="width:6.5in;height:3.63889in" /></p>
<p><em>Figure ??: Forestplot of E1 ~Task Completion Session Count</em></p>
<h4 id="model-2-e1-learning-session-count">4.2.3.2 Model 2: E1 ~ Learning Session Count</h4>
<p>In contrast, a simple linear regression with learning session count as the sole predictor yielded a less interesting story. The model did not achieve the assumptions of statistical power on 0.8 with an alpha of 0.05 (Actual Cohen’s f-squared = 0.007 &lt; Target Cohen’s f-squared = 0.092). The model model (F(1, 85) = 0.62, p = .435) was not statistically significant. In addition, the sessions classified as learning explained less than 1% of variance in E1 (r-squared = 0.007, adjusted r-squared = -0.004). The regression coefficient was positive but not statistically significant (beta = 0.06, SE = 0.08, t = 0.79, p = 0.435, 95% CI [-0.10, 0.23]). Learning session count was not a predictor of exam performance when considered in isolation.</p>
<p><img src="./publish/thesis2/html/media/image6.png" style="width:6.5in;height:3.84722in" /><br />
<em>Figure ??: Forestplot of E1 ~ Learning Session Count</em></p>
<h4 id="model-3-e1-learning-session-count-task-completion-session-count">4.2.3.3 Model 3: E1 ~ Learning Session Count + Task Completion Session Count</h4>
<p>A different pattern emerged when both task completion count and learning session count were factored into the multiple regression model. The full model was statistically significant (F(2, 84) = 9.76.p-value = 0.0002), and met assumptions for statistical power (Actual Cohen’s f-squared = 0.233 &gt; Target Cohen’s f-squared = 0.115). The two variable model explained 18.9% of the variance in E1 (r-squared= 0.189, adjusted r-squared=0.169), this was 48.8% more than Model 1 alone (r-squared = 0.127, adjusted r-squared = 0.117). In addition both effects were enhanced when compared to their bivariate analyses, becoming significant predictors of E1 when included together in the model.</p>
<p>Task completion session counts (beta = -0.54, SE = 0.125, <em>t</em> = -4.33, <em>p</em> &lt; .000, 95% CI [-0.79, -0.292) showed a 31% increase in effect size compared to model 1 (beta = -0.41). For Learning session counts (beta = 0.206, SE = 0.081, <em>t</em> = -2.53, <em>p</em> = 0.013, 95% CI [0.044, 0.367), there was no significant effect present in Model 2. Learning session counts were significant in Model 3 and the beta coefficient went from 0.06 to 0.206, representing an over 200% increase in effect size. Considering the change in r-squared from Model 1 to Model 3 was significant (delta r-squared = 0.189 - 0.127 = 0.062) indicating that the addition of learning session counts provided 6% more unique explanatory power beyond task completion sessions alone.</p>
<p><img src="./publish/thesis2/html/media/image20.png" style="width:6.5in;height:3.63889in" /></p>
<p><em>Figure ??: Forestplot of E1 ~ Task Completion Session Count + Learning Session Count</em></p>
<h3 id="summary-of-findings-for-rq1">4.2.4 Summary of Findings for RQ1</h3>
<p>The comparison across models revealed a mutual suppression effect between task completion session counts and learning session counts when predicting midterm exam scores (E1). The predictors shared variance that obscures their individual relationships with the outcome when considered in isolation. As explained earlier, the beta coefficient for task completion sessions increased in magnitude from -0.41 to -0.54, while learning sessions transformed from a non-significant predictor (p = 0.435) to a significant one (p = 0.013), with the coefficient increasing more than threefold (from 0.06 to 0.206). This pattern indicates that each predictor shares variance with the other that was irrelevant to predicting E1, but each also possessed unique variance that was predictive.</p>
<p>When included in the same model, they suppress each other's irrelevant variance while allowing their unique contributions to emerge. The substantial increase in explained variance, that is the change in r-squared from Model 1 to Model 3 = 0.062 and from Model 2 to Model 3 = .182, demonstrates that both session types contributed non-redundant information about exam performance. This mutual suppression effect could be interpreted as participants who engaged in more task completion sessions tended to have fewer learning sessions and vice versa.</p>
<table>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Independent<br />
Variables</strong></th>
<th><strong>r-squared</strong></th>
<th><strong>Adjusted<br />
r-squared</strong></th>
<th><strong>F</strong></th>
<th><strong>p</strong></th>
<th><strong>Delta<br />
R-squared<br />
Model 1</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Task Completion</td>
<td>0.127</td>
<td>0.117</td>
<td>12.35</td>
<td>&lt; 0.001</td>
<td>—</td>
</tr>
<tr class="even">
<td>2</td>
<td>Learning</td>
<td>0.007</td>
<td>-0.004</td>
<td>0.62</td>
<td>= 0.435</td>
<td>—</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Task Completion + Learning</td>
<td>0.189</td>
<td>0.169</td>
<td>9.76</td>
<td>&lt; 0.001</td>
<td>+ 0.62</td>
</tr>
</tbody>
</table>
<p><em>Table ??: Hierarchical Model Comparison</em></p>
<p>The effects were in opposition to one another, the learning session counts had a positive effect (beta = 0.206) while the task completion count had a negative effect (beta = -0.54) that was more than twice as influential as learning session counts. This suggests that which seems quite obvious: these are competing strategies for academic success.</p>
<p>The significant negative relationship observed between task completion session counts and midterm exam scores (E1) suggests participants who focus more on task completion tend to perform worse on exams. This aligns with the notion that surface-level engagement activities such as answer seeking are less effective for deep learning and retention.</p>
<p>Conversely, the significant positive relationship that existed between learning session counts and midterm exam scores (E1) suggests that students who engage more in learning-oriented sessions tend to perform better on exams. The effect size for learning session counts is smaller than that for task completion session counts; this may be due other confounding factors not accounted for in the model, such as prior knowledge, inclination to grasp the material, study habits, external support such as tutoring, or test anxiety.</p>
<h3 id="conclusion-for-rq1">4.2.5 Conclusion for RQ1</h3>
<p>The multiple regression analysis of model E1 ~ Task Completion Count + Learning Session Count (F(2, 84) = 9.76, p-value &lt;0.001) provided strong statistical evidence to reject the null hypothesis. Both hypothesized relationships were confirmed: learning sessions positively predicted exam performance (beta = 0.21, p = 0.013), while task completion sessions negatively predicted performance (beta = -0.54, p &lt; 0.001).</p>
<p>I can conclude the type of LLM engagement and not merely the frequency of use predicts academic performance. Learning-focused engagement supports academic success as measured through exam scores, while task-completion-focused engagement undermines it. Although the findings did reject the null hypothesis and establish significant correlations between LLM usage patterns and midterm exam performance, we cannot conclude that task completion causes lower exam performance or that learning sessions contribute to higher exam scores. Regardless. opposing directional relationships when combined with the mutual suppression effect provide strong evidence that the way we engage with LLMs has unique associations with learning outcomes.</p>
<h2 id="findings-for-rq2">4.3 Findings for RQ2</h2>
<h3 id="model-overview-for-rq2">4.3.1 Model Overview for RQ2</h3>
<p>This research question investigated the impact of in-context learning on midterm exam scores (E1) via a between-subjects experimental design. Participants were randomly assigned to the control (n=48) or treatment group (n=39) during the six week intervention. The assignment instructions were added to LLM’s context of participants in the treatment group, meaning the LLM was aware of assignment-related questions. Specifically, I examined whether the treatment improved with academic outcomes on the exam.</p>
<p>The regression model used to test the hypothesis where the control/treatment groups were coded with a dummy variable (control=0, treatment=1).</p>
<p><strong>Model 0: E1 ~ Treatment</strong></p>
<p><strong>Alternate Hypothesis for H2</strong></p>
<p>The mean student learning performance score (E1) for the treatment group, which used the context-aware AI, will be statistically significantly higher than the mean score for the control group.</p>
<p><strong>Null Hypothesis for H2</strong></p>
<p>There is no statistically significant difference in the mean student learning performance scores (E1) between the treatment group and the control group.</p>
<h3 id="regression-assumption-tests-for-the-rq2-model">4.3.2 Regression Assumption Tests for the RQ2 Model</h3>
<p>An assessment of ordinary least squares (OLS) regression assumptions was conducted prior to interpreting the findings of the model for RQ2. These checks assured the validity and reliability of the model estimates. Because the independent variable was binary (control vs treatment) the linearity assumption was satisfied. Linear regression estimates the difference in means between control and treatment which was a linear relationship.</p>
<p>The Durbin-Watson statistic (DW = 2.11) falls within the acceptable range and is sufficiently close to the ideal value of 2.0, indicating no evidence of autocorrelation. This confirms that the residuals are independent, which is expected given the cross-sectional nature of the data where each student's exam score represents an independent observation.</p>
<p>To check for homoscedasticity, or constant variance the Breusch-Pagan was administered. The test yielded a non-significant result (chi-square = 0.081, p = 0.776), indicating that the residuals exhibited homoscedasticity across the range of fitted values. This suggests that the variance of exam scores (E1) remained constant across both the control and treatment groups.</p>
<p>To check for normality of residuals, a Shapiro-Wilk test for normality was executed. The test result was non-significant (W = 0.98, p = 0.167), suggesting that the residuals approximated a normal distribution. This finding was corroborated visually via a histogram of residuals and a Q-Q plot to observe adherence to a normal distribution.</p>
<p><img src="./publish/thesis2/html/media/image9.png" style="width:6.5in;height:2.30556in" /><br />
<em>Figure ??: Evidence of normality among the residuals of E1 ~ Treatment</em></p>
<p>The variance inflation factor (VIF) for the predictor variable was 1.00, indicating the complete absence of multicollinearity. This makes sense as multicollinearity is primarily a concern in multiple regression models, and this diagnostic confirmed no issues with the single predictor in the current model.<br />
Given that all diagnostic tests indicated satisfaction of OLS regression assumptions, the model was deemed appropriate for interpretation.</p>
<h3 id="regression-analysis-for-the-rq2">4.3.3 Regression Analysis for the RQ2 </h3>
<p>The regression model revealed the control group achieved a mean E1 score of 30.23 points (SE = 0.87) while participants in the treatment group, who had access to the context-aware AI, scored an average of 2.05 points higher than the control group (beta = 2.05, SE = 1.30). However, this difference did not reach statistical significance (p-value = 0.119, 95% CI [-0.54, 4.64]). In addition, the confidence interval crossed the zero boundary, further indicating that the true population difference could plausibly be null.</p>
<p><img src="./publish/thesis2/html/media/image21.png" style="width:6.5in;height:3.97222in" /><br />
<em>Figure ??: Forestplot of E1 ~ Treatment</em></p>
<p>The overall model fit was weak, with the group assignment variable accounting for only 2.8% of the variance in E1 scores (r-squared = 0.028, adjusted <em>r-squared</em> = .017). The <em>F</em>-statistic for the model was not significant (<em>F</em>(1, 85) = 2.485, <em>p</em> = 0.119), indicating that the model did not explain a statistically significant proportion of variance in exam performance.</p>
<p>Finally the model did not meet statistical power assumptions for a power of 0.8 and an alpha of 0.05, with Cohen’s f-squared failing to meet the target (Actual Cohen’s f-squared = 0.029 &lt; Target Cohen’s f-squared = 0.092). In addition, Cohen's f-squared = 0.029 falls below the threshold for a small effect size, meaning even if the results were statistically significant, the practical significance of the group differences would be minimal.</p>
<p>The findings suggest that making the AI assistant aware of assignment-related questions through in-Context learning did not translate into measurably improved exam performance for students in this sample (N = 87, Control=48, Treatment=39). The lack of significant effect may indicate that the in-context learning manipulation, while theoretically sound, did not sufficiently alter student learning behaviors or outcomes in ways that manifested on the midterm examination.</p>
<p>Given the findings of RQ1, specifically the impact of learning session count and task completion counts on midterm exam scores, a more comprehensive examination of variables was warranted. In the next sections I attempt to identify the impacts of the LLM use as independent variables as covariates, in a mediation analysis, in a moderation analysis and then finally summarizing a clear picture of the impact of the treatment on midterm exam scores.</p>
<h4 id="multiple-regression-with-usage-as-covariates-for-rq2">4.3.3.1 Multiple Regression with usage as Covariates for RQ2</h4>
<p>To investigate whether usage patterns might be obscuring or influencing the treatment effect, a multiple regression analysis was conducted including learning session count and task completion session count as additional predictors alongside treatment condition.</p>
<p><strong>Model 1: E1 ~ Treatment + Learning Session Count + Task Completion Session Count</strong></p>
<p>The multiple regression model was statistically significant (<em>F</em>(3, 83) = 8.641, <em>p</em> &lt; .001) and explained 23.8% of variance in E1 scores (r-squared = 0.238, adjusted r-squared = 0.210). This represents a substantial improvement over the simple regression model 0, confirming as we did in RQ1 that AI usage patterns account for considerable variance in exam performance. Cohen's f-squared = 0.312 indicates a medium-to-large effect size for the overall model, and was above the threshold of statistical power at 0.8 with alpha 0.05 (Target Cohen’s f-squared = 0.131).</p>
<p>When controlling for learning session count and task completion count, the treatment effect became statistically significant (beta = 2.74, <em>SE</em> = 1.18, <em>t</em>(83) = 2.32, <em>p</em> = 0.023, 95% CI [0.39, 5.09]). Participants in the treatment group scored approximately 2.74 points higher on the midterm exam E1 compared to control group students with equivalent LLM usage patterns.</p>
<p>Both learning session count and task completion count demonstrated significant relationships with exam performance when controlling for the treatment. Learning session count remained a significant positive predictor (beta = 0.232, <em>SE</em> = 0.08, <em>t</em>(83) = 2.90, <em>p</em> = 0.005, 95% CI [0.73, -0.39]), while task completion count demonstrated a significant negative relationship (beta = -0.569, <em>SE</em> = 0.12, <em>t</em>(83) = -4.65, <em>p</em> &lt; 0.001, 95% CI [-0.81, -0.33]).</p>
<p><img src="./publish/thesis2/html/media/image14.png" style="width:6.5in;height:3.58333in" /><br />
<em>Figure ??: Forestplot of E1 ~ Treatment + Learning Session Count + Task Completion Session Count</em></p>
<p>The emergence of statistical significance after controlling for covariates indicates a suppression effect. Although AI usage patterns themselves were not significantly affected by treatment, their inclusion as covariates removed confounding variance that had masked the true treatment effect. In other words, the treatment did matter but its effect was hidden until we controlled for usage.This pattern suggests the treatment's impact on exam performance operates independently of behavioral AI usage patterns indicated by learning session count and task completion count.</p>
<h4 id="mediation-analysis-for-rq2">4.3.3.2 Mediation Analysis for RQ2</h4>
<p>By answering RQ1, I discovered that AI usage impacted midterm exam scores.To explore whether the treatment effect operated through changes in student AI usage behaviors, specifically learning session count and task completion count, a mediation analysis was employed. So for the mediation analysis we would need two additional regressions to complete the story:</p>
<p><strong>Model 2a: Learning Session Count ~ Treatment</strong></p>
<p><strong>Model 2b: Task Completion Count ~ Treatment</strong></p>
<p>For Model 2a, the linear regression revealed treatment group assignment did not significantly predict learning session counts (beta = -1.79, SE = 1.73, t(85) = -1.04, p = 0.304, r-squared = 0.012). In addition, Cohen's f-squared = 0.126 failed to reach its target of 0.0923. Descriptively, control group students engaged in more learning sessions (mean = 7.79, std = 8.18) compared to treatment group students (mean = 6.00, std = 7.85), though this difference was not statistically reliable.</p>
<p>Model 2a was also found to be statistically insignificant. Treatment group assignment did not significantly predict task completion session counts (beta = 0.47, SE = 1.14, t(85) = 0.42, p = 0.678, r-squared = 0.002). Descriptively, treatment group students demonstrated slightly higher task completion activity (mean = 4.41, std = 5.30) relative to control group students (mean = 3.93, std = 5.24), but once again this difference was not significant.</p>
<p>The absence of significant relationships between treatment assignment and AI usage patterns as indicated by models 2a and 2b precludes mediation. According to established mediation frameworks <a href="https://www.zotero.org/google-docs/?yKu1Fr">(Hayes, 2009)</a> an indirect effect cannot exist without the predictor significantly affecting the proposed mediator. Therefore, the treatment effect on exam performance does not operate through changes in student usage behaviors as indicated through task competition session count and learning session count.</p>
<h4 id="moderation-analysis-for-rq2">4.3.3.3 Moderation Analysis for RQ2</h4>
<p>In order to complete the picture, two moderation analyses examined whether treatment conditions altered the strength of relationships between AI usage patterns and exam performance. The following regression models tested whether the treatment moderated the relationship between learning sessions and exam performance, and task completion sessions and exam performance respectively.</p>
<p><strong>Model 3a: E1 ~ Treatment + Learning Session Count + Treatment x Learning Session Count</strong></p>
<p><strong>Model 3b: Model 3a: E1 ~ Treatment + Task Completion Session Count + Treatment x Task Completion Session Count</strong></p>
<p>Model 3a tested whether context-aware AI moderated the relationship between learning session count and exam performance. The overall model was not statistically significant (F(3, 83) = 1.289, p = 0.284, r-squared = 0.045). The moderation term of treatment x learning session count was non-significant (beta = 0.11, SE = 0.17, t(83) = 0.675, p = 0.502, 95% CI [-0.22, 0.44]), indicating that treatment did not moderate the learning session count relationship. Neither the main effect of treatment (beta = 1.44, p = 0.407) nor learning sessions (beta = 0.03, p = 0.769) were significant in this model.</p>
<p>The second model 3b examined whether treatment moderated the relationship between task completion sessions and exam performance. The overall model was statistically significant (F(3, 83) = 5.350, p = 0.002, r-squared = 0.162), but the moderation term of treatment x task completion session count remained non-significant (beta = -0.07, SE = 0.24, t(83) = -0.316, p = 0.753, 95% CI [-0.54, 0.39]). The model revealed a significant main effect for task completion sessions (beta = -0.39, SE = 0.16, t(83) = -2.470, p = 0.016), confirming the negative relationship with exam scores. The treatment main effect approached but did not reach significance (beta = 2.56, SE = 1.57, p = 0.107).</p>
<p>The absence of statistically significant moderation effects among treatment x learning session count and treatment x task completion count indicated the treatment condition did not moderate the relationships between AI usage patterns and exam performance. Learning sessions benefit participant exam scores equally regardless of control or treatment. Likewise, task completion session counts show a negative relationship with midterm exam score across both conditions.</p>
<h3 id="summary-of-findings-for-rq2">4.3.4 Summary of Findings for RQ2</h3>
<p>The following table summarizes the analytical findings for RQ2.</p>
<table>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Analysis Type</strong></th>
<th><strong>Key Finding</strong></th>
<th><strong>Conclusion</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>Simple Regression<br />
E1 ~ Treatment</td>
<td>Treatment not significant p=0.119</td>
<td>Initial null finding. This led to further investigations.</td>
</tr>
<tr class="even">
<td>1</td>
<td>Multiple Regression<br />
E1 ~ Treatment + Learning SC + Task Completion SC</td>
<td>Treatment significant when controlling for usage (beta = 2.74, p = 0.023)</td>
<td>Suppression effect revealed through covariates.</td>
</tr>
<tr class="odd">
<td>2a</td>
<td>Mediation<br />
Learning SC~ Treatment</td>
<td>No mediation path<br />
p = 0.304</td>
<td>Treatment doesn’t change Learning SC</td>
</tr>
<tr class="even">
<td>2b</td>
<td><p>Mediation</p>
<p>Task Completion SC ~ Treatment</p></td>
<td>No mediation path<br />
p = 0.678</td>
<td>Treatment doesn’t change Task Completion SC</td>
</tr>
<tr class="odd">
<td>3a</td>
<td>Moderation<br />
E1 ~ Treatment + Learning SC+<br />
Treatment x Learning SC</td>
<td>No moderation<br />
p = 0.502</td>
<td>Treatment doesn’t amplify learning SC</td>
</tr>
<tr class="even">
<td>3b</td>
<td>Moderation<br />
E1 ~ Treatment +<br />
Task Completion SC +<br />
Treatment x Task Completion SC</td>
<td>No moderation<br />
p = 0.753</td>
<td>Treatment doesn’t amplify task completion SC</td>
</tr>
</tbody>
</table>
<p><em>Table ??: Analytical findings for RQ2. Note: SC is an abbreviation for Session Count</em></p>
<p>The initial model 0 revealed a null effect. The true effect was masked by other factors, specifically learning session count and task completion session count, which were identified as significant in RQ1. As a result, model 1 was more precise accounting for 23.8% of variance in E1 scores, compared to just 2.8% for model 0. When these confounding associations were removed through statistical control, the true treatment benefit became apparent: context-aware AI provides approximately a 2.74-point advantage on E1 exam scores independent of AI usage patterns as specified by learning session count and task completion session count.</p>
<p>To better understand if the independent variables learning session count and task completion session count were more than just covariates, a mediation and moderation analysis were conducted. No mediation was identified, meaning the treatment had no effect on learning session count or task completion session count. This is a key finding as it explains that the covariates are not the mechanism through which the treatment worked. In addition, no moderation was identified meaning the treatment effect did not change due to learning session count or task completion count.This implies the treatment effect is generalizable and does not depend on the covariates. In summary, the treatment has a significant and consistent effect on the outcome (E1) after controlling for pre-existing factors of learning session count and task completion session count. These factors are important predictors of the outcome, but they do not explain the mechanism of the treatment, nor do they alter its effectiveness.</p>
<h3 id="conclusion-for-rq2">4.3.5 Conclusion for RQ2</h3>
<p>Because the relationship between in-context learning and midterm exam performance exhibited a suppression effect, initial simple regression analysis failed to detect a significant treatment effect (<em>p</em> = .119). However, when controlling for participant AI usage patterns, the treatment effect became statistically significant (beta = 2.74, <em>p</em> = 0.023), revealing that context-aware AI provides a meaningful performance advantage independent of learning session or task completion session counts. This was the basis of the multiple regression analysis of model 1 E1 ~ Treatment + Task Completion Count + Learning Session Count (F(3, 83) = 8.641, p-value &lt;0.001) provided strong statistical evidence to reject the null hypothesis.</p>
<p>Therefore, based on model 1, I rejected the null hypothesis and concluded that in-context learning significantly improves student exam performance. Participants using the treatment of context-aware AI scored approximately 2.74 points higher on the midterm exam (E1) compared to students using standard AI, when controlling for usage patterns (<em>p</em> = 0.023). This effect operated independently of how participants AI use was classified for learning session or task completion session counts as verified based on mediation and moderation findings. Findings suggest context awareness enhanced the quality of AI-mediated learning interactions rather than their quantity.</p>
<h2 id="findings-for-rq3">4.4 Findings for RQ3</h2>
<h3 id="model-overview-for-rq3">4.4.1 Model Overview for RQ3</h3>
<p>This research question investigated the relationship between the quantity of participant sessions classified as “learning” and “task completion” from the D1 dataset and their computational literacy scores (C1, C2) among the participants (N = 87).</p>
<p>The regression model used to test the hypothesis had learning session count and task completion count as independent variables, with the post-intervention computational literacy score C2 as the dependent variable. To account the impact of test-retest between C1 and C2, the pre-test C1 was added to the model as part of an analysis of covariance.</p>
<p><strong>Model 1: C2 ~ Learning Session Count + Task Completion Count + C1</strong></p>
<p>In addition, I also conducted a supplementary analysis using the change in diagnostic scores (C2 - C1) as the dependent variable to examine whether LLM usage patterns predicted the magnitude of improvement. Because change scores often have low reliability when pre- and post-test correlations are high <a href="https://www.zotero.org/google-docs/?2wjkP7">(Cronbach &amp; Furby, 1970)</a>, this was considered a complementary approach.</p>
<p><strong>Model 1a: C2-C1 ~ Learning Session Count + Task Completion Count</strong></p>
<p><strong>Alternate Hypothesis for H3</strong></p>
<p>The quantity and classification of participant sessions in the D1 dataset are significantly correlated with their computational literacy scores (C1, C2). Specifically:</p>
<ol type="1">
<li><blockquote>
<p>There is a positive correlation between the quantity of sessions classified as “learning” and computational literacy scores.</p>
</blockquote></li>
<li><blockquote>
<p>There is a negative correlation between the quantity of sessions classified as "task completion" and computational literacy scores.</p>
</blockquote></li>
</ol>
<p><strong>Null Hypothesis for H3</strong></p>
<p>There is no statistically significant correlation between the quantity of participant sessions from the D1 dataset, whether classified as "learning" or "task completion," and their computational literacy scores (C1, C2).</p>
<p>Both conditions 1 and 2 must be satisfied to reject the null hypothesis for H3.</p>
<h3 id="regression-assumption-tests-for-the-rq3-model">4.4.2 Regression Assumption Tests for the RQ3 Model</h3>
<p>A comprehensive assessment of ordinary least squares (OLS) regression assumptions was conducted prior to interpreting the findings of the model for RQ1. These checks assured the validity and reliability of the model estimates.</p>
<p>Preliminary diagnostic testing revealed heteroscedasticity (Breusch-Pagan: chi-squared = 12.58, p-value = 0.006), indicating non-constant error variance across the range of fitted values. This was confirmed visually in the regression plots which exhibit a cone-like shape. To ensure valid statistical inference in the presence of heteroscedasticity, I report all regression results using heteroscedasticity-consistent standard errors. This approach provides robust hypothesis tests without requiring the homoscedasticity assumption and yielded accurate p-values and confidence intervals even when error variance was not constant.</p>
<p>To check for linearity, partial regression plots were examined to assess the linear relationship between the dependent variable (C2) and each predictor variable while controlling for other predictors in the model. Visual inspection of these plots indicated approximate linear relationships with the heteroscedasticity visually apparent. The session-oriented independent variables had fewer observations as the counts of the participants’ sessions increased, as was expected since this data is commonly right-skewed.</p>
<p><img src="./publish/thesis2/html/media/image4.png" style="width:6.5in;height:4.48611in" /><br />
<em>Figure ??: Partial regression plots when holding the other constant of C2 ~ Task Completion Session Count + Learning Session Count. + C1 with visual confirmation of heteroscedasticity.</em></p>
<p>The plot of residuals versus fitted values showed symmetrical, randomly scattered points. Heteroscedasticity can be observed on the right end of the graph. The Locally Estimated Scatterplot Smoothing (LOESS) curve was relatively flat and hovering about horizontal.</p>
<p><img src="./publish/thesis2/html/media/image1.png" style="width:5.16667in;height:3.46875in" /><br />
<em>Figure ??: Residuals vs Fitted Values for C2 ~ Learning Session Count + Task Completion Session Count + C1</em></p>
<p>Other than Breusch-Pagan, all other tests were satisfied. The Durbin-Watson statistic (DW = 2.22) was within the acceptable range, falling close to the ideal value of 2.0. This indicated no substantial autocorrelation among residuals, thereby satisfying the assumption of independence.</p>
<p>The Shapiro-Wilk test indicated that residuals were normally distributed (W = 0.99, p-value = 0.551). Since p&gt;=0.05 we fail to reject the null hypothesis of normality. This was corroborated visually via a histogram of residuals and the Q-Q plot, both of which demonstrated close adherence to the normal distribution.</p>
<p><img src="./publish/thesis2/html/media/image10.png" style="width:6.5in;height:2.27778in" /><br />
<em>Figure ??: Evidence of normality among the residuals of C2 ~ Task Completion Session Count + Learning Session Count + C1</em></p>
<p>The final check was for multicollinearity. The variance inflation factor (VIF) test was performed for all independent variables with and no variable reporting a VIF &gt; 1.27 indicating no problematic multicollinearity. The VIF for C1 was 1.07.</p>
<h3 id="regression-analysis-for-rq3-accounting-for-test-retest">4.4.3 Regression Analysis for RQ3 Accounting for test-retest</h3>
<p>When observing descriptive statistics, computational literacy scores increased with statistical significance from pre-test (C1: mean = 20.57, std = 4.71) to post-test (C2: mean = 21.44, std = 4.78), paired t(86) = -2.67, p-value = .009, Cohen's d = 0.29. This indicates modest yet statistically significant learning gains from pre to post intervention. As expected with the same instrument, C1 and C2 scores were strongly correlated (Pearson’s r = 0.79, p &lt; .001), suggesting high test-retest reliability and relative stability in individual rankings despite overall improvement. Statistically significant improvement combined with strong test-retest correlation justifies the analysis of covariance approach. This preserved information about individual trajectories while controlling for baseline differences.</p>
<p>Model 1: C2 ~ Task Completion Session Count + Learning Session Count + C1 was highly significant, <em>F</em>(3, 83) = 54.66, <em>p</em> &lt; 0.001, and explained 65.8% of variance in post-test (C2) scores (r-squared = 0.658, adjusted r-squared = 0.646). The effect size was large by Cohen’s f-squared measures (Cohen's f-squared = 1.93), and well above the target (Cohen’s f-squared = 0.1314) for statistical power at 0.8 with an alpha of 0.05.</p>
<p>As expected, baseline computational literacy was a strong predictor of post-test scores (beta = 0.76, SE = 0.08, <em>z</em> = 10.07, <em>p</em> &lt; .001, 95% CI [0.61, 0.91]), accounting for the majority of explained variance in the model. For every 1-point increase in baseline scores, post-test scores increased by 0.76 points on average, when holding task completion session count and learning session count constant.</p>
<p>After controlling for baseline ability in C1, task completion session count remained a significant negative predictor (beta = -0.17, <em>SE</em> = 0.07, <em>z</em> = -2.52, <em>p</em> = 0.012, 95% CI [-0.30, -0.04]). Each additional task completion session was associated with a 0.17-point decrement in post-test computational literacy scores, when holding baseline literacy (C1) and learning sessions counts constant. This suggests that greater task-oriented LLM use predicted lower final achievement than would be expected given participants' initial competency levels. Notably, this effect remained statistically significant after correction for heteroscedasticity via robust-standard errors, demonstrating its robustness.</p>
<p>Learning session count showed a positive relationship that approached but did not achieve conventional statistical significance (beta = 0.07, SE = 0.04, <em>z</em> = 1.76, <em>p</em> = 0.078, 95% CI [-0.01, 0.14]). The direction of the relationship aligned with the hypothesized positive effect in addition to the findings in RQ1 and RQ2. The marginal p-value suggests a potential beneficial relationship between learning-oriented LLM usage and computational literacy that warrants investigation with larger samples or more sensitive measures.</p>
<p><img src="./publish/thesis2/html/media/image2.png" style="width:6.5in;height:3.59722in" /><br />
<em>Figure ??: Forestplot of C2 ~ Learning Session Count + Task Completion Session Count + C1</em></p>
<h4 id="supplementary-analysis-change-in-scores">4.4.3.1 Supplementary Analysis Change in Scores</h4>
<p>This supplementary analysis examined whether LLM usage patterns predicted the magnitude of improvement rather than final standing relative to baseline. I ran a supplementary regression analysis with change scores (C2 - C1) as the dependent variable and learning session count and task completion session count a the independent variables</p>
<p><strong>Model 1a: C2 - C1 ~ Learning Session Count + Task Completion Count</strong></p>
<p>This model satisfied all regression assumptions (Breusch-Pagan: p = 0.75; Durbin-Watson: DW = 1.97; Shapiro-Wilk: p = 0.64; Multicollinearity: VIF scores = 1.19) but was not statistically significant overall, F(2, 84) = 1.57, p = 0.214, r-squared = 0.036, adjusted r-squared = .013.</p>
<p>Neither task completion session count (beta = -0.11, SE = 0.07, t = -1.67, <em>p</em> = .099, 95% CI [-0.25, 0.02]) nor learning session count (beta = 0.05, <em>SE</em> = 0.04, t = 1.22, p = .225, 95% CI [-0.03, 0.14]) significantly predicted gain scores (C2-C1). While task completion trended in the negative direction and learning sessions trended positive which was consistent with other models, neither relationship reached statistical significance.</p>
<p>The divergence between the Model 1 (ANCOVA) and Model 1a (Change in Scores) provided important interpretive information. Task completion session count usage predicted where participants ended up relative to their baseline potential (Model 1) but not how much they improved in absolute terms (Model 1a). This suggests that the effect operates through relative positioning from the baseline C1 score.</p>
<h3 id="summary-of-findings-for-rq3">4.4.4 Summary of Findings for RQ3</h3>
<p>The following table summarizes the analytical findings for RQ2.</p>
<table>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Independent Variables</strong></th>
<th><strong>Dependent Variables</strong></th>
<th><strong>Findings</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1 (ANCOVA)</td>
<td>Learning Session Count +<br />
Task Completion Session Count + C1</td>
<td>C2</td>
<td>Statistical significance except for Learning Session Count p = 0.08</td>
</tr>
<tr class="even">
<td>1a (Change in Scores)</td>
<td>Learning Session Count +<br />
Task Completion Session Count</td>
<td>C2-C1</td>
<td>Not statistically significant.</td>
</tr>
</tbody>
</table>
<p>Table ??: Summary of Findings for RQ3</p>
<p>Research Question 3 asked: "What is the relationship between large language model use and computational literacy?" The analysis from model 1 provided a nuanced answer as the relationship depends on how students use LLMs<strong>.</strong> Task Completion sessions demonstrated a robust negative association with computational literacy (beta = -0.17, <em>p</em> = .012). Participants who engaged in more task completion sessions achieved lower C2 scores than would be predicted from their baseline ability. This suggests that instrumental LLM use may constrain computational literacy relative to potential.</p>
<p>In the same model 1, learning session counts showed a positive trend (beta = 0.07, p = 0.078) yet did not reach statistical significance. While this finding is inconclusive, the direction aligns with theoretical predictions and suggests that exploratory engagement may support computational literacy development in ways that instrumental task completion does not.</p>
<p>A supplemental analysis of change in scores between C1 and C1 revealed no statistical significance (r-squared = 0.036, p-value = 0.214), indicating learning session count and task completion session count have no influence in the change in scores.</p>
<h3 id="conclusion-for-rq3">4.4.5 Conclusion for RQ3</h3>
<p>The alternate hypothesis (H3) proposed that LLM usage patterns would be significantly correlated with computational literacy scores, specifically predicting a negative correlation for task completion sessions and a positive correlation for learning sessions. H3 was partially supported.</p>
<p>The predicted negative relationship between task completion session count and computational literacy was confirmed (beta = -0.17, p = 0.012), and this effect remained significant after controlling for baseline ability and correcting for heteroscedasticity. I therefore rejected the null hypothesis with respect to task completion sessions.</p>
<p>However, the predicted positive relationship between learning sessions and computational literacy did not achieve statistical significance at the conventional alpha = .05 threshold (beta = 0.07, p = 0.078). While the direction of the relationship was aligned with predictions and other analysis as part of RQ1 and RQ2, the marginal p-value suggests a possible effect, I cannot reject the null hypothesis with respect to learning sessions at conventional significance levels. The relationship remains suggestive and warrants further investigation with perhaps larger samples.</p>
<p>Given that the overall regression of model 1 was highly significant (F(3, 83) = 54.66, p &lt; .001) and task completion demonstrated a robust relationship, we conclude that LLM usage patterns are significantly related to computational literacy, though the nature of this relationship varies by usage type.</p>
<h2 id="overall-summary-of-findings">4.5 Overall Summary of Findings</h2>
<p>TODO: Sum up the findings broadly</p>
<p>RQ1 Met</p>
<p>RQ2 Met</p>
<p>RQ3 Partially Met</p>
<h1 id="summary"><strong>5.0 Summary</strong></h1>
<p>TODO: THIS SECTION NEEDS TO BE WRITTEN. HERE ARE SOME NOTES</p>
<p>NOTE: Important not to draw too broad a conclusion from the findings. Consider the scope and participant audience. This study took place over 1 semester due to an accelerated timeline. Further study is required to better understand the nuances observed. For the most part the findings are promising and demonstrate a true effect.between understanding the chat interactions and performing the analysis. Plans to continue this study in future semesters to strengthen these findings further.</p>
<p>Also consider this thesis very much an exploration. Hence the wide set of methods and broad research questions. Trying to understand what among AI use actually influences grades. To some degree, I seem to have figured that out.</p>
<h1 id="section-1"><br />
</h1>
<h1 id="references"><strong>References</strong></h1>
<blockquote>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., &amp; Henighan, T. (2020). <em>Language Models are Few-Shot Learners</em>.</p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Brennan, K., &amp; Resnick, M. (2012). <em>New frameworks for studying and assessing the development of computational thinking</em>.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Breusch, T. S., &amp; Pagan, A. R. (1979). A simple test for heteroscedasticity and random coefficient variation. <em>Econometrica: Journal of the Econometric Society</em>, 1287–1294.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., &amp; Henighan, T. (2020). <em>Language Models are Few-Shot Learners</em>.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Cambaz, D., &amp; Zhang, X. (2024). Use of AI-driven Code Generation Models in Teaching and Learning Programming: A Systematic Literature Review. <em>Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1</em>, 172–178. https://doi.org/10.1145/3626252.3630958</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Christensen, C. M., McDonald, R., Altman, E. J., &amp; Palmer, J. E. (2018). Disruptive Innovation: An Intellectual History and Directions for Future Research. <em>Journal of Management Studies</em>, <em>55</em>(7), 1043–1078. https://doi.org/10.1111/joms.12349</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2013). <em>Applied multiple regression/correlation analysis for the behavioral sciences</em>. Routledge.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Creswell, J. W., &amp; Creswell, J. D. (2017). <em>Research design: Qualitative, quantitative, and mixed methods approaches</em>. Sage publications.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. <em>Psychometrika</em>, <em>16</em>(3), 297–334.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Cronbach, L. J., &amp; Furby, L. (1970). How we should measure" change": Or should we? <em>Psychological Bulletin</em>, <em>74</em>(1), 68.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Durbin, J., &amp; Watson, G. (1971). Testing for serial correlation in least squares regression. III. <em>Biometrika</em>, <em>58</em>(1), 1–19.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Edwards, J. R., &amp; Lambert, L. S. (2007). Methods for integrating moderation and mediation: A general analytical framework using moderated path analysis. <em>Psychological Methods</em>, <em>12</em>(1), 1.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Farrar, D. E., &amp; Glauber, R. R. (1967). Multicollinearity in regression analysis: The problem revisited. <em>The Review of Economic and Statistics</em>, 92–107.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Finnie-Ansley, J., Denny, P., Becker, B. A., Luxton-Reilly, A., &amp; Prather, J. (2022). The robots are coming: Exploring the implications of openai codex on introductory programming. <em>Proceedings of the 24th Australasian Computing Education Conference</em>, 10–19.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Gillioz, A., Casas, J., Mugellini, E., &amp; Khaled, O. A. (2020). <em>Overview of the Transformer-based Models for NLP Tasks</em>. 179–183. https://doi.org/10.15439/2020F20</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Hackl, V., Müller, A. E., Granitzer, M., &amp; Sailer, M. (2023). Is GPT-4 a reliable rater? Evaluating consistency in GPT-4’s text ratings. <em>Frontiers in Education</em>, <em>8</em>, 1272229.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Halevy, A., Norvig, P., &amp; Pereira, F. (2009). The Unreasonable Effectiveness of Data. <em>IEEE Intelligent Systems</em>, <em>24</em>(2), 8–12. https://doi.org/10.1109/MIS.2009.36</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Hassan, M., Chen, Y., Denny, P., &amp; Zilles, C. (2025). On Teaching Novices Computational Thinking by Utilizing Large Language Models Within Assessments. <em>Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1</em>, 471–477. https://doi.org/10.1145/3641554.3701906</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Hayes, A. F. (2009). Beyond Baron and Kenny: Statistical mediation analysis in the new millennium. <em>Communication Monographs</em>, <em>76</em>(4), 408–420.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Horn, M. B. (2024, June 3). <em>What does Disruptive Innovation Theory have to say about AI? - Christensen Institute</em>. https://www.christenseninstitute.org/blog/what-does-disruptive-innovation-say-about-ai/</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Huitema, B. (2011). <em>The analysis of covariance and alternatives: Statistical methods for experiments, quasi-experiments, and single-case studies</em>. John Wiley &amp; Sons.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Krippendorff, K. (2011). <em>Computing Krippendorff’s alpha-reliability</em>.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Krippendorff, K. (2018). <em>Content analysis: An introduction to its methodology</em>. Sage publications.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">MacKinnon, J. G., &amp; White, H. (1985). Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties. <em>Journal of Econometrics</em>, <em>29</em>(3), 305–325.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Margulieux, L. E., Prather, J., Reeves, B. N., Becker, B. A., Cetin Uzun, G., Loksa, D., Leinonen, J., &amp; Denny, P. (2024). Self-Regulation, Self-Efficacy, and Fear of Failure Interactions with How Novices Use LLMs to Solve Programming Problems. <em>Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1</em>, 276–282. https://doi.org/10.1145/3649217.3653621</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Maxwell, S. E., Delaney, H. D., &amp; Kelley, K. (2017). <em>Designing experiments and analyzing data: A model comparison perspective</em>. Routledge.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Neuendorf, K. A. (2017). <em>The content analysis guidebook</em>. sage.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Osborne, J. W., &amp; Waters, E. (2002). Four assumptions of multiple regression that researchers should always test. <em>Practical Assessment, Research, and Evaluation</em>, <em>8</em>(1).</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Păvăloaia, V.-D., &amp; Necula, S.-C. (2023). Artificial Intelligence as a Disruptive Technology—A Systematic Literature Review. <em>Electronics</em>, <em>12</em>(5), 1102. https://doi.org/10.3390/electronics12051102</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Pilny, A., McAninch, K., Slone, A., &amp; Moore, K. (2024). From manual to machine: Assessing the efficacy of large language models in content analysis. <em>Communication Research Reports</em>, <em>41</em>(2), 61–70.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Prather, J., Pettit, R., Becker, B. A., Denny, P., Loksa, D., Peters, A., Albrecht, Z., &amp; Masci, K. (2019). First Things First: Providing Metacognitive Scaffolding for Interpreting Problem Prompts. <em>Proceedings of the 50th ACM Technical Symposium on Computer Science Education</em>, 531–537. https://doi.org/10.1145/3287324.3287374</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Prather, J., Reeves, B., Leinonen, J., MacNeil, S., Randrianasolo, A. S., Becker, B., Kimmel, B., Wright, J., &amp; Briggs, B. (2024). <em>The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers</em> (No. arXiv:2405.17739). arXiv. http://arxiv.org/abs/2405.17739</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Prather, J., Reeves, B. N., Denny, P., Becker, B. A., Leinonen, J., Luxton-Reilly, A., Powell, G., Finnie-Ansley, J., &amp; Santos, E. A. (2024). “It’s Weird That it Knows What I Want”: Usability and Interactions with Copilot for Novice Programmers. <em>ACM Transactions on Computer-Human Interaction</em>, <em>31</em>(1), 1–31. https://doi.org/10.1145/3617367</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Raudenbush, S. W., &amp; Bryk, A. S. (2002). <em>Hierarchical linear models: Applications and data analysis methods</em> (Vol. 1). sage.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Román-González, M., Pérez-González, J.-C., &amp; Jiménez-Fernández, C. (2017). Which cognitive abilities underlie computational thinking? Criterion validity of the Computational Thinking Test. <em>Computers in Human Behavior</em>, <em>72</em>, 678–691. https://doi.org/10.1016/j.chb.2016.08.047</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Shanahan, M. (2024). Talking about Large Language Models. <em>Communications of the ACM</em>, <em>67</em>(2), 68–79. https://doi.org/10.1145/3624724</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Shaphiro, S., &amp; Wilk, M. (1965). An analysis of variance test for normality. <em>Biometrika</em>, <em>52</em>(3), 591–611.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Shute, V. J., Sun, C., &amp; Asbell-Clarke, J. (2017). Demystifying computational thinking. <em>Educational Research Review</em>, <em>22</em>, 142–158. https://doi.org/10.1016/j.edurev.2017.09.003</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Vaz, S., Falkmer, T., Passmore, A. E., Parsons, R., &amp; Andreou, P. (2013). The case for using the repeatability coefficient when calculating test–retest reliability. <em>PloS One</em>, <em>8</em>(9), e73990.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., &amp; Le, Q. V. (2022). <em>Finetuned Language Models Are Zero-Shot Learners</em> (No. arXiv:2109.01652). arXiv. http://arxiv.org/abs/2109.01652</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., &amp; Zhou, D. (2022). <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>.</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Xiao, Z., Yuan, X., Liao, Q. V., Abdelghani, R., &amp; Oudeyer, P.-Y. (2023). <em>Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding</em>. 75–78. https://doi.org/10.1145/3581754.3584136</a></p>
<p><a href="https://www.zotero.org/google-docs/?q7bqaa">Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). <em>A Survey of Large Language Models</em> (No. arXiv:2303.18223). arXiv. http://arxiv.org/abs/2303.18223</a> stensen, C. M., McDonald, R., Altman, E. J., &amp; Palmer, J. E. (2018). Disruptive Innovation: An Intellectual History and Directions for Future Research. <em>Journal of Management Studies</em>, <em>55</em>(7), 1043–1078. https://doi.org/10.1111/joms.12349</p>
</blockquote>
</body>
</html>
